{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목차\n",
    "\n",
    "전체 코드\n",
    "\n",
    "1. 필요한 라이브러리 설치\n",
    "2. 라이브러리 임포트 및 장치 설정\n",
    "3. 데이터 준비\n",
    "4. 데이터 전처리\n",
    "5. 데이터셋 및 데이터로더 생성\n",
    "6. 모델 로드 및 설정\n",
    "7. 모델 학습\n",
    "8. 모델 저장\n",
    "9. 챗봇 실행을 위한 모델 로드\n",
    "10. 챗봇 응답 생성 함수 정의\n",
    "11. 챗봇 실행\n",
    "\n",
    "추가 고려 사항\n",
    "\n",
    "결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 필요한 라이브러리 설치\n",
    "- 먼저, 필요한 라이브러리를 설치합니다. 이 셀을 실행하여 transformers, torch, sentencepiece 등을 설치하세요.\n",
    "- !pip install transformers torch sentencepiece\n",
    "\n",
    "#### 2. 라이브러리 임포트 및 장치 설정\n",
    "- 필요한 라이브러리를 임포트하고, GPU 사용 가능 여부를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 장치: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 장치 설정 (GPU 사용 가능 시 GPU, 아니면 CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 데이터 준비\n",
    "- 음식 추천 챗봇을 위한 질문-답변 데이터를 준비\n",
    "- 실제 프로젝트에서는 더 많은 데이터 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = [\n",
    "    {\"input\": \"배가 고픈데 뭐 먹을까?\", \"output\": \"한식은 어때요? 따뜻한 국밥이 좋을 것 같아요.\"},\n",
    "    {\"input\": \"점심 추천해줘\", \"output\": \"가벼운 샐러드나 샌드위치는 어떠세요?\"},\n",
    "    {\"input\": \"저녁 뭐 먹을지 고민돼\", \"output\": \"맛있는 파스타나 스테이크는 어떠신가요?\"},\n",
    "    {\"input\": \"달콤한 디저트 먹고 싶어\", \"output\": \"초콜릿 케이크나 아이스크림을 추천드려요.\"},\n",
    "    {\"input\": \"시원한 거 마시고 싶어\", \"output\": \"아이스 아메리카노나 레몬 에이드를 드셔보세요.\"},\n",
    "    {\"input\": \"매운 음식이 땡겨\", \"output\": \"매운 떡볶이나 불닭볶음면은 어떠세요?\"},\n",
    "    {\"input\": \"간단하게 먹을 거 없어?\", \"output\": \"김밥이나 샌드위치를 드셔보세요.\"},\n",
    "    {\"input\": \"한국 전통 음식 추천해줘\", \"output\": \"비빔밥이나 불고기를 추천합니다.\"},\n",
    "    {\"input\": \"중국 음식이 먹고 싶어\", \"output\": \"짜장면이나 탕수육은 어떠세요?\"},\n",
    "    {\"input\": \"일식 좋아해\", \"output\": \"스시나 우동을 드셔보세요.\"},\n",
    "    {\"input\": \"양식 먹고 싶어\", \"output\": \"스테이크나 피자를 추천합니다.\"},\n",
    "    {\"input\": \"채식주의자야\", \"output\": \"채소 샐러드나 두부 요리를 드셔보세요.\"},\n",
    "    {\"input\": \"디저트 카페 추천해줘\", \"output\": \"근처에 있는 베이커리 카페는 어떠세요?\"},\n",
    "    {\"input\": \"건강식이 필요해\", \"output\": \"현미밥과 야채로 구성된 식단을 추천합니다.\"},\n",
    "    {\"input\": \"해산물 요리 먹고 싶어\", \"output\": \"회나 해물파전은 어떠세요?\"},\n",
    "    {\"input\": \"친구들이랑 먹을 거 추천해줘\", \"output\": \"치킨이나 피자를 시켜 드세요.\"},\n",
    "    {\"input\": \"새로운 맛집 없어?\", \"output\": \"요즘 핫한 수제 버거 가게를 추천합니다.\"},\n",
    "    {\"input\": \"간식 추천해줘\", \"output\": \"과일이나 요거트는 어떠신가요?\"},\n",
    "    {\"input\": \"더운 날씨에 뭐 먹지?\", \"output\": \"시원한 냉면이나 팥빙수를 드셔보세요.\"},\n",
    "    {\"input\": \"감기 걸렸어\", \"output\": \"따뜻한 죽이나 수프를 드시는 게 좋겠어요.\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 데이터 전처리\n",
    "- 입력과 출력 텍스트를 하나의 시퀀스로 결합하고, 레이블을 생성\n",
    "- 입력 부분은 손실 계산에서 제외하기 위해 -100으로 마스킹 \n",
    "- 또한, max_length를 명시적으로 설정하여 경고를 방지\n",
    "##### 설명:\n",
    "- 패딩 토큰과 종료 토큰 구분: tokenizer.pad_token과 tokenizer.eos_token이 동일한지 확인하고, 동일하다면 pad_token을 <pad>로 추가하여 구분, \n",
    "- 이는 attention_mask가 정확하게 생성되도록 함\n",
    "- 토크나이징 시 max_length 설정: max_length=128을 설정하여 입력 시퀀스의 최대 길이를 제한\n",
    "- 이는 경고를 방지하고, 모델이 예상치 못한 긴 입력을 처리하지 않도록 함\n",
    "- 레이블 마스킹: 입력 부분을 -100으로 마스킹하여 모델이 손실을 계산할 때 해당 부분을 무시하도록 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\gnn_snn_torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token='</s>',\n",
    "    eos_token='</s>',\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>'\n",
    ")\n",
    "\n",
    "# pad_token이 없는 경우 추가\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "for pair in train_data:\n",
    "    input_text = f\"질문: {pair['input']}\\n답변: \"\n",
    "    output_text = pair['output'] + tokenizer.eos_token\n",
    "    full_text = input_text + output_text\n",
    "    inputs.append(full_text)\n",
    "\n",
    "# 토크나이즈 및 텐서 변환 (max_length를 명시적으로 설정)\n",
    "encodings = tokenizer(\n",
    "    inputs,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,  # 필요에 따라 조정 가능\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids = encodings.input_ids\n",
    "attention_mask = encodings.attention_mask\n",
    "labels = input_ids.clone()\n",
    "\n",
    "# 입력 부분 마스킹\n",
    "for i in range(len(train_data)):\n",
    "    input_text = f\"질문: {train_data[i]['input']}\\n답변: \"\n",
    "    input_ids_i = tokenizer.encode(input_text, add_special_tokens=False)\n",
    "    input_length = len(input_ids_i)\n",
    "    labels[i, :input_length] = -100  # 입력 부분 마스킹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 데이터셋 및 데이터로더 생성\n",
    "- 파이토치의 TensorDataset과 DataLoader를 사용하여 데이터를 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(input_ids, encodings.attention_mask, labels)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 모델 로드 및 설정\n",
    "- 사전 학습된 KoGPT2 모델을 로드하고, 패딩 토큰을 반영하도록 설정\n",
    "\n",
    "##### 설명:\n",
    "- 토큰 임베딩 크기 조정: 패딩 토큰을 추가한 후, 모델의 토큰 임베딩 크기를 조정하여 새로운 토큰을 반영\n",
    "- 이를 통해 패딩 토큰을 제대로 인식할 수 있도록 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\gnn_snn_torch\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # pad_token 추가 후 토큰 임베딩 크기 조정\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 모델 학습\n",
    "- 모델을 학습\n",
    "- 에포크 수를 늘려 모델이 데이터를 충분한 학습 필요\n",
    "##### 설명:\n",
    "- 에포크 증가: 에포크 수를 늘려 모델이 데이터를 충분히 학습하도록 처리 필요\n",
    "- 손실 값 모니터링: 각 에포크마다 평균 손실 값을 출력하여 학습 상태를 모니터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 1/100 진행 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\gnn_snn_torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 1 완료, 평균 손실 값: 4.9320\n",
      "에포크 2/100 진행 중...\n",
      "에포크 2 완료, 평균 손실 값: 1.2204\n",
      "에포크 3/100 진행 중...\n",
      "에포크 3 완료, 평균 손실 값: 0.3581\n",
      "에포크 4/100 진행 중...\n",
      "에포크 4 완료, 평균 손실 값: 0.2390\n",
      "에포크 5/100 진행 중...\n",
      "에포크 5 완료, 평균 손실 값: 0.1685\n",
      "에포크 6/100 진행 중...\n",
      "에포크 6 완료, 평균 손실 값: 0.0716\n",
      "에포크 7/100 진행 중...\n",
      "에포크 7 완료, 평균 손실 값: 0.0680\n",
      "에포크 8/100 진행 중...\n",
      "에포크 8 완료, 평균 손실 값: 0.0482\n",
      "에포크 9/100 진행 중...\n",
      "에포크 9 완료, 평균 손실 값: 0.0529\n",
      "에포크 10/100 진행 중...\n",
      "에포크 10 완료, 평균 손실 값: 0.0653\n",
      "에포크 11/100 진행 중...\n",
      "에포크 11 완료, 평균 손실 값: 0.0292\n",
      "에포크 12/100 진행 중...\n",
      "에포크 12 완료, 평균 손실 값: 0.0296\n",
      "에포크 13/100 진행 중...\n",
      "에포크 13 완료, 평균 손실 값: 0.0240\n",
      "에포크 14/100 진행 중...\n",
      "에포크 14 완료, 평균 손실 값: 0.0163\n",
      "에포크 15/100 진행 중...\n",
      "에포크 15 완료, 평균 손실 값: 0.0212\n",
      "에포크 16/100 진행 중...\n",
      "에포크 16 완료, 평균 손실 값: 0.0359\n",
      "에포크 17/100 진행 중...\n",
      "에포크 17 완료, 평균 손실 값: 0.0361\n",
      "에포크 18/100 진행 중...\n",
      "에포크 18 완료, 평균 손실 값: 0.0214\n",
      "에포크 19/100 진행 중...\n",
      "에포크 19 완료, 평균 손실 값: 0.0219\n",
      "에포크 20/100 진행 중...\n",
      "에포크 20 완료, 평균 손실 값: 0.0053\n",
      "에포크 21/100 진행 중...\n",
      "에포크 21 완료, 평균 손실 값: 0.0060\n",
      "에포크 22/100 진행 중...\n",
      "에포크 22 완료, 평균 손실 값: 0.0047\n",
      "에포크 23/100 진행 중...\n",
      "에포크 23 완료, 평균 손실 값: 0.0028\n",
      "에포크 24/100 진행 중...\n",
      "에포크 24 완료, 평균 손실 값: 0.0022\n",
      "에포크 25/100 진행 중...\n",
      "에포크 25 완료, 평균 손실 값: 0.0027\n",
      "에포크 26/100 진행 중...\n",
      "에포크 26 완료, 평균 손실 값: 0.0013\n",
      "에포크 27/100 진행 중...\n",
      "에포크 27 완료, 평균 손실 값: 0.0014\n",
      "에포크 28/100 진행 중...\n",
      "에포크 28 완료, 평균 손실 값: 0.0020\n",
      "에포크 29/100 진행 중...\n",
      "에포크 29 완료, 평균 손실 값: 0.0011\n",
      "에포크 30/100 진행 중...\n",
      "에포크 30 완료, 평균 손실 값: 0.0010\n",
      "에포크 31/100 진행 중...\n",
      "에포크 31 완료, 평균 손실 값: 0.0008\n",
      "에포크 32/100 진행 중...\n",
      "에포크 32 완료, 평균 손실 값: 0.0008\n",
      "에포크 33/100 진행 중...\n",
      "에포크 33 완료, 평균 손실 값: 0.0008\n",
      "에포크 34/100 진행 중...\n",
      "에포크 34 완료, 평균 손실 값: 0.0009\n",
      "에포크 35/100 진행 중...\n",
      "에포크 35 완료, 평균 손실 값: 0.0007\n",
      "에포크 36/100 진행 중...\n",
      "에포크 36 완료, 평균 손실 값: 0.0008\n",
      "에포크 37/100 진행 중...\n",
      "에포크 37 완료, 평균 손실 값: 0.0007\n",
      "에포크 38/100 진행 중...\n",
      "에포크 38 완료, 평균 손실 값: 0.0006\n",
      "에포크 39/100 진행 중...\n",
      "에포크 39 완료, 평균 손실 값: 0.0006\n",
      "에포크 40/100 진행 중...\n",
      "에포크 40 완료, 평균 손실 값: 0.0008\n",
      "에포크 41/100 진행 중...\n",
      "에포크 41 완료, 평균 손실 값: 0.0007\n",
      "에포크 42/100 진행 중...\n",
      "에포크 42 완료, 평균 손실 값: 0.0007\n",
      "에포크 43/100 진행 중...\n",
      "에포크 43 완료, 평균 손실 값: 0.0006\n",
      "에포크 44/100 진행 중...\n",
      "에포크 44 완료, 평균 손실 값: 0.0006\n",
      "에포크 45/100 진행 중...\n",
      "에포크 45 완료, 평균 손실 값: 0.0005\n",
      "에포크 46/100 진행 중...\n",
      "에포크 46 완료, 평균 손실 값: 0.0005\n",
      "에포크 47/100 진행 중...\n",
      "에포크 47 완료, 평균 손실 값: 0.0005\n",
      "에포크 48/100 진행 중...\n",
      "에포크 48 완료, 평균 손실 값: 0.0006\n",
      "에포크 49/100 진행 중...\n",
      "에포크 49 완료, 평균 손실 값: 0.0005\n",
      "에포크 50/100 진행 중...\n",
      "에포크 50 완료, 평균 손실 값: 0.0005\n",
      "에포크 51/100 진행 중...\n",
      "에포크 51 완료, 평균 손실 값: 0.0006\n",
      "에포크 52/100 진행 중...\n",
      "에포크 52 완료, 평균 손실 값: 0.0005\n",
      "에포크 53/100 진행 중...\n",
      "에포크 53 완료, 평균 손실 값: 0.0004\n",
      "에포크 54/100 진행 중...\n",
      "에포크 54 완료, 평균 손실 값: 0.0004\n",
      "에포크 55/100 진행 중...\n",
      "에포크 55 완료, 평균 손실 값: 0.0004\n",
      "에포크 56/100 진행 중...\n",
      "에포크 56 완료, 평균 손실 값: 0.0003\n",
      "에포크 57/100 진행 중...\n",
      "에포크 57 완료, 평균 손실 값: 0.0004\n",
      "에포크 58/100 진행 중...\n",
      "에포크 58 완료, 평균 손실 값: 0.0004\n",
      "에포크 59/100 진행 중...\n",
      "에포크 59 완료, 평균 손실 값: 0.0003\n",
      "에포크 60/100 진행 중...\n",
      "에포크 60 완료, 평균 손실 값: 0.0003\n",
      "에포크 61/100 진행 중...\n",
      "에포크 61 완료, 평균 손실 값: 0.0003\n",
      "에포크 62/100 진행 중...\n",
      "에포크 62 완료, 평균 손실 값: 0.0003\n",
      "에포크 63/100 진행 중...\n",
      "에포크 63 완료, 평균 손실 값: 0.0004\n",
      "에포크 64/100 진행 중...\n",
      "에포크 64 완료, 평균 손실 값: 0.0004\n",
      "에포크 65/100 진행 중...\n",
      "에포크 65 완료, 평균 손실 값: 0.0003\n",
      "에포크 66/100 진행 중...\n",
      "에포크 66 완료, 평균 손실 값: 0.0003\n",
      "에포크 67/100 진행 중...\n",
      "에포크 67 완료, 평균 손실 값: 0.0003\n",
      "에포크 68/100 진행 중...\n",
      "에포크 68 완료, 평균 손실 값: 0.0003\n",
      "에포크 69/100 진행 중...\n",
      "에포크 69 완료, 평균 손실 값: 0.0003\n",
      "에포크 70/100 진행 중...\n",
      "에포크 70 완료, 평균 손실 값: 0.0003\n",
      "에포크 71/100 진행 중...\n",
      "에포크 71 완료, 평균 손실 값: 0.0003\n",
      "에포크 72/100 진행 중...\n",
      "에포크 72 완료, 평균 손실 값: 0.0003\n",
      "에포크 73/100 진행 중...\n",
      "에포크 73 완료, 평균 손실 값: 0.0003\n",
      "에포크 74/100 진행 중...\n",
      "에포크 74 완료, 평균 손실 값: 0.0003\n",
      "에포크 75/100 진행 중...\n",
      "에포크 75 완료, 평균 손실 값: 0.0003\n",
      "에포크 76/100 진행 중...\n",
      "에포크 76 완료, 평균 손실 값: 0.0002\n",
      "에포크 77/100 진행 중...\n",
      "에포크 77 완료, 평균 손실 값: 0.0002\n",
      "에포크 78/100 진행 중...\n",
      "에포크 78 완료, 평균 손실 값: 0.0003\n",
      "에포크 79/100 진행 중...\n",
      "에포크 79 완료, 평균 손실 값: 0.0003\n",
      "에포크 80/100 진행 중...\n",
      "에포크 80 완료, 평균 손실 값: 0.0003\n",
      "에포크 81/100 진행 중...\n",
      "에포크 81 완료, 평균 손실 값: 0.0002\n",
      "에포크 82/100 진행 중...\n",
      "에포크 82 완료, 평균 손실 값: 0.0002\n",
      "에포크 83/100 진행 중...\n",
      "에포크 83 완료, 평균 손실 값: 0.0003\n",
      "에포크 84/100 진행 중...\n",
      "에포크 84 완료, 평균 손실 값: 0.0002\n",
      "에포크 85/100 진행 중...\n",
      "에포크 85 완료, 평균 손실 값: 0.0002\n",
      "에포크 86/100 진행 중...\n",
      "에포크 86 완료, 평균 손실 값: 0.0002\n",
      "에포크 87/100 진행 중...\n",
      "에포크 87 완료, 평균 손실 값: 0.0002\n",
      "에포크 88/100 진행 중...\n",
      "에포크 88 완료, 평균 손실 값: 0.0002\n",
      "에포크 89/100 진행 중...\n",
      "에포크 89 완료, 평균 손실 값: 0.0002\n",
      "에포크 90/100 진행 중...\n",
      "에포크 90 완료, 평균 손실 값: 0.0002\n",
      "에포크 91/100 진행 중...\n",
      "에포크 91 완료, 평균 손실 값: 0.0002\n",
      "에포크 92/100 진행 중...\n",
      "에포크 92 완료, 평균 손실 값: 0.0002\n",
      "에포크 93/100 진행 중...\n",
      "에포크 93 완료, 평균 손실 값: 0.0002\n",
      "에포크 94/100 진행 중...\n",
      "에포크 94 완료, 평균 손실 값: 0.0002\n",
      "에포크 95/100 진행 중...\n",
      "에포크 95 완료, 평균 손실 값: 0.0002\n",
      "에포크 96/100 진행 중...\n",
      "에포크 96 완료, 평균 손실 값: 0.0002\n",
      "에포크 97/100 진행 중...\n",
      "에포크 97 완료, 평균 손실 값: 0.0002\n",
      "에포크 98/100 진행 중...\n",
      "에포크 98 완료, 평균 손실 값: 0.0002\n",
      "에포크 99/100 진행 중...\n",
      "에포크 99 완료, 평균 손실 값: 0.0002\n",
      "에포크 100/100 진행 중...\n",
      "에포크 100 완료, 평균 손실 값: 0.0002\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  # 에포크 수 조정 가능\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"에포크 {epoch+1}/{epochs} 진행 중...\")\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids_batch = batch[0].to(device)\n",
    "        attention_mask_batch = batch[1].to(device)\n",
    "        labels_batch = batch[2].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids_batch,\n",
    "            attention_mask=attention_mask_batch,\n",
    "            labels=labels_batch\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"에포크 {epoch+1} 완료, 평균 손실 값: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. 모델 저장\n",
    "- 학습이 완료된 모델과 토크나이저를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./food_chatbot_model\\\\tokenizer_config.json',\n",
       " './food_chatbot_model\\\\special_tokens_map.json',\n",
       " './food_chatbot_model\\\\vocab.json',\n",
       " './food_chatbot_model\\\\merges.txt',\n",
       " './food_chatbot_model\\\\added_tokens.json',\n",
       " './food_chatbot_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./food_chatbot_model\")\n",
    "tokenizer.save_pretrained(\"./food_chatbot_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. 챗봇 실행을 위한 모델 로드\n",
    "- 저장된 모델과 토크나이저를 로드하여 챗봇을 실행할 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./food_chatbot_model\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./food_chatbot_model\")\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 챗봇 응답 생성 함수 정의\n",
    "- 챗봇의 응답을 생성하는 함수를 정의\n",
    "- 여기서 Zero-Shot과 Few-Shot 학습 방식을 적용\n",
    "##### 설명:\n",
    "- Zero-Shot: 예시 없이 직접 질문에 답변할 때 사용, 모델의 사전 학습된 지식을 활용\n",
    "- Few-Shot: 몇 가지 예시를 제공하여 모델이 응답의 패턴을 학습하도록 유도 \n",
    "    - 이를 통해 응답의 일관성과 정확성을 높일 수 있다.\n",
    "\n",
    "- max_length 설정: tokenizer.encode()와 model.generate()에서 max_length를 명시적으로 설정하여 경고를 방지하고, 입력 시퀀스의 최대 길이 제한\n",
    "- 응답 추출: 답변: 이후의 텍스트를 추출하여 반환,\n",
    "    - 만약 답변:이 없을 경우, 기본적인 응답을 제공\n",
    "- 디버깅 추가: output을 출력하여 모델이 어떤 텍스트를 생성하고 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text, mode='few-shot'):\n",
    "    if mode == 'zero-shot':\n",
    "        prompt = f\"질문: {input_text}\\n답변: \"\n",
    "    elif mode == 'few-shot':\n",
    "        # Few-Shot을 위해 몇 가지 예시를 추가\n",
    "        few_shot_examples = (\n",
    "            \"질문: 배가 고픈데 뭐 먹을까?\\n답변: 한식은 어때요? 따뜻한 국밥이 좋을 것 같아요.\\n\"\n",
    "            \"질문: 점심 추천해줘\\n답변: 가벼운 샐러드나 샌드위치는 어떠세요?\\n\"\n",
    "            \"질문: 저녁 뭐 먹을지 고민돼\\n답변: 맛있는 파스타나 스테이크는 어떠신가요?\\n\"\n",
    "        )\n",
    "        prompt = f\"{few_shot_examples}질문: {input_text}\\n답변: \"\n",
    "    else:\n",
    "        prompt = f\"질문: {input_text}\\n답변: \"\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt', truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=128,  # 적절한 max_length 설정\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.92,\n",
    "            temperature=0.6,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "    output = tokenizer.decode(\n",
    "        output_ids[0],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True  # 경고를 제거하기 위해 추가\n",
    "    )\n",
    "    \n",
    "    # 디버깅을 위한 출력\n",
    "    print(f\"DEBUG: Generated Output: {output}\")\n",
    "\n",
    "    # 답변 부분만 추출 (마지막 '답변:' 이후 텍스트)\n",
    "    if \"답변:\" in output:\n",
    "        response = output.rsplit(\"답변:\", 1)[-1].strip()\n",
    "    else:\n",
    "        response = \"죄송합니다. 이해하지 못했어요.\"\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. 챗봇 실행\n",
    "- 챗봇을 실행하여 질문에 답변 처리\n",
    "##### 설명:\n",
    "- 종료 조건: 사용자가 '종료'를 입력하면 대화를 종료\n",
    "- 응답 생성: generate_response 함수에 mode를 전달하여 Zero-Shot 또는 Few-Shot 방식을 선택\n",
    "    - few-shot 방식을 사용하도록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "음식 추천 챗봇입니다. 종료하려면 '종료'를 입력하세요.\n",
      "DEBUG: Generated Output: 질문: 점심추천해줘\n",
      "답변: 짬밥이나 불고기를 추천합니다.\n",
      "챗봇: 짬밥이나 불고기를 추천합니다.\n",
      "DEBUG: Generated Output: 질문: 짬밥말고 다른거\n",
      "답변: 짬뽕이나 불닭볶음면은 어떠세요?\n",
      "챗봇: 짬뽕이나 불닭볶음면은 어떠세요?\n",
      "DEBUG: Generated Output: 질문: 다른거\n",
      "답변: 칵테나 레몬 에이드를 드셔보세요.\n",
      "챗봇: 칵테나 레몬 에이드를 드셔보세요.\n",
      "DEBUG: Generated Output: 질문: 점심에 레모?\n",
      "답변: 짬밥이나 샌드위치를 드셔보세요.\n",
      "챗봇: 짬밥이나 샌드위치를 드셔보세요.\n",
      "DEBUG: Generated Output: 질문: 다른거 없나?\n",
      "답변: 쫄깃한 국밥이 좋을 것 같아요.\n",
      "챗봇: 쫄깃한 국밥이 좋을 것 같아요.\n",
      "챗봇: 대화를 종료합니다. 좋은 하루 되세요!\n"
     ]
    }
   ],
   "source": [
    "print(\"음식 추천 챗봇입니다. 종료하려면 '종료'를 입력하세요.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"사용자: \")\n",
    "    if user_input.strip().lower() == \"종료\":\n",
    "        print(\"챗봇: 대화를 종료합니다. 좋은 하루 되세요!\")\n",
    "        break\n",
    "    # mode를 'zero-shot' 또는 'few-shot'으로 설정\n",
    "    response = generate_response(user_input, mode='zero-shot')\n",
    "    print(f\"챗봇: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_snn_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
