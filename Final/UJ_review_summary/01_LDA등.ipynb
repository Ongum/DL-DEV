{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] 지정된 프로시저를 찾을 수 없습니다. Error loading \"c:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\__init__.py:137\u001b[0m\n\u001b[0;32m    135\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    136\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] 지정된 프로시저를 찾을 수 없습니다. Error loading \"c:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "C:\\Users\\G-01\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KoBERT 또는 KC-BERT 모델과 토크나이저 로드\n",
    "model_name = \"monologg/kobert\"  # KoBERT 모델 사용\n",
    "# model_name = \"beomi/kcbert-base\"  # KC-BERT를 사용하려면 이 줄을 사용하세요.\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "data = pd.read_csv('../외국음식전문점.csv')\n",
    "data = data.dropna(subset=['content'])  # 결측값 처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 텍스트 전처리\n",
    "data['content'] = data['content'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")  # 한글 외 문자 제거\n",
    "data['content'] = data['content'].str.replace(\"\\s+\", \" \")  # 불필요한 공백 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 클래스를 정의하여 데이터를 관리\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        tokens = {key: val.squeeze(0) for key, val in tokens.items()}\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DataLoader로 배치 처리 (병렬 처리 강화)\n",
    "def create_dataloader(data, tokenizer, batch_size=64, num_workers=None):\n",
    "    if num_workers is None:\n",
    "        num_workers = multiprocessing.cpu_count()  # 모든 CPU 코어 사용\n",
    "    dataset = TextDataset(data, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 데이터를 토큰화하고 임베딩을 얻는 함수\n",
    "def get_bert_embeddings(dataloader, model, device):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    scaler = GradScaler()  # Mixed Precision 사용 설정\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Mixed Precision을 사용하여 속도 최적화\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                embeddings.extend(batch_embeddings)\n",
    "    return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 생성 (모든 CPU 코어를 활용해 데이터 로딩 병렬화)\n",
    "dataloader = create_dataloader(data['content'].tolist(), tokenizer, batch_size=64, num_workers=multiprocessing.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G-01\\AppData\\Local\\Temp\\ipykernel_15032\\592206904.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Mixed Precision 사용 설정\n",
      "C:\\Users\\G-01\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 계산\n",
    "embeddings = get_bert_embeddings(dataloader, model, device)\n",
    "\n",
    "# LDA를 사용한 주제 모델링\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=None)\n",
    "tfidf = tfidf_vectorizer.fit_transform(data['content'])\n",
    "\n",
    "lda = LDA(n_components=3, random_state=42)  # 3개의 주제 (맛, 분위기, 서비스)\n",
    "lda.fit(tfidf)\n",
    "\n",
    "# 각 문서의 주제 분포를 확인하고 주제를 할당\n",
    "topic_distributions = lda.transform(tfidf)\n",
    "data['lda_topic'] = topic_distributions.argmax(axis=1)\n",
    "\n",
    "# 결과 확인\n",
    "for i, text in enumerate(data['content'].head(10)):\n",
    "    print(f\"Text: {text} | LDA Topic: {data['lda_topic'].iloc[i]}\")\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 각 주제별로 문서 확인\n",
    "for topic in range(3):\n",
    "    print(f\"\\nTopic {topic}:\")\n",
    "    print(data[data['lda_topic'] == topic]['content'].head(5))\n",
    "\n",
    "# 클러스터링의 품질을 평가하기 위해 실루엣 점수를 계산합니다.\n",
    "silhouette_avg = silhouette_score(embeddings, data['lda_topic'])\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
