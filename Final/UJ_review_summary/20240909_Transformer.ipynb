{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GPU 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   1347 MiB |  15653 MiB |   3277 GiB |   3276 GiB |\n",
      "|       from large pool |    987 MiB |  15473 MiB |   2817 GiB |   2816 GiB |\n",
      "|       from small pool |    360 MiB |    519 MiB |    459 GiB |    459 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   1347 MiB |  15653 MiB |   3277 GiB |   3276 GiB |\n",
      "|       from large pool |    987 MiB |  15473 MiB |   2817 GiB |   2816 GiB |\n",
      "|       from small pool |    360 MiB |    519 MiB |    459 GiB |    459 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   1347 MiB |  15651 MiB |   3274 GiB |   3273 GiB |\n",
      "|       from large pool |    987 MiB |  15471 MiB |   2816 GiB |   2815 GiB |\n",
      "|       from small pool |    360 MiB |    519 MiB |    458 GiB |    458 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  16200 MiB |  16200 MiB |  17208 MiB |   1008 MiB |\n",
      "|       from large pool |  15694 MiB |  15694 MiB |  16526 MiB |    832 MiB |\n",
      "|       from small pool |    506 MiB |    522 MiB |    682 MiB |    176 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   1658 MiB |   4583 MiB |   2751 GiB |   2749 GiB |\n",
      "|       from large pool |   1656 MiB |   4580 MiB |   2238 GiB |   2237 GiB |\n",
      "|       from small pool |      1 MiB |     40 MiB |    512 GiB |    512 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     692    |    1314    |    9443 K  |    9442 K  |\n",
      "|       from large pool |     153    |     712    |     375 K  |     375 K  |\n",
      "|       from small pool |     539    |     789    |    9068 K  |    9067 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     692    |    1314    |    9443 K  |    9442 K  |\n",
      "|       from large pool |     153    |     712    |     375 K  |     375 K  |\n",
      "|       from small pool |     539    |     789    |    9068 K  |    9067 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     458    |     486    |     569    |     111    |\n",
      "|       from large pool |     205    |     225    |     228    |      23    |\n",
      "|       from small pool |     253    |     261    |     341    |      88    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      86    |     151    |    4128 K  |    4128 K  |\n",
      "|       from large pool |      79    |     113    |     255 K  |     255 K  |\n",
      "|       from small pool |       7    |      55    |    3873 K  |    3873 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G-01\\AppData\\Local\\Temp\\ipykernel_2876\\625024579.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reducedData['content'] = reducedData['content'].fillna('')\n",
      "C:\\Users\\G-01\\AppData\\Local\\Temp\\ipykernel_2876\\625024579.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reducedData['clean_content'] = reducedData['content'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# T5 모델과 토크나이저 로드\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# GPU 메모리 확인\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.memory_summary(device=device))\n",
    "\n",
    "# 리뷰 데이터를 읽어옴\n",
    "file_path = '../외국음식전문점_테스트.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터의 첫 1000개 행만 사용\n",
    "reducedData = data.iloc[:1000, :]\n",
    "\n",
    "# 결측치 또는 NaN 데이터를 처리하여, NaN 값을 빈 문자열로 대체\n",
    "reducedData['content'] = reducedData['content'].fillna('')\n",
    "\n",
    "# 데이터 전처리: 불필요한 공백, 특수문자 제거\n",
    "def clean_text(text):\n",
    "    # 불필요한 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9., ]', '', text)\n",
    "    return text\n",
    "\n",
    "# 모든 리뷰 텍스트 전처리\n",
    "reducedData['clean_content'] = reducedData['content'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G-01\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 데이터셋 준비\n",
    "def preprocess_data(texts, max_length=512):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# 마스킹 함수\n",
    "def create_mask(input_ids):\n",
    "    return (input_ids != tokenizer.pad_token_id).float()\n",
    "\n",
    "# 전처리된 데이터로부터 입력과 마스크 생성\n",
    "train_inputs, train_masks = preprocess_data(reducedData['clean_content'])\n",
    "train_labels, _ = preprocess_data(reducedData['content'])\n",
    "\n",
    "# 배치 처리 및 데이터셋 준비\n",
    "batch_size = 4  # 배치 크기 설정\n",
    "accumulation_steps = 4  # 그래디언트 축적 단계\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 혼합 정밀도 학습을 위한 스케일러 설정\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [14:27<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 1.291033679485321\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [14:34<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 2: 0.12410829064249992\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [15:02<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 3: 0.058464961193501946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 학습 루프\n",
    "num_epochs = 3  # 에포크 수 설정\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch_inputs, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        with autocast():  # 혼합 정밀도 학습 사용\n",
    "            outputs = model(input_ids=batch_inputs, attention_mask=batch_masks, labels=batch_labels)\n",
    "            loss = outputs.loss / accumulation_steps  # 그래디언트 축적을 위해 손실 나누기\n",
    "\n",
    "        scaler.scale(loss).backward()  # 스케일된 그래디언트 적용\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)  # 옵티마이저 업데이트\n",
    "            scaler.update()  # 스케일러 업데이트\n",
    "            optimizer.zero_grad()  # 그래디언트 초기화\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Loss after epoch {epoch + 1}: {total_loss / len(train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 언제나줄서야먹을수있는테이블좀 늘려주세요 아님확장\n",
      "Summary: \n",
      "Original: 목동에서 젤좋아하는 곳인데 오늘 고기도 여러점인데도  너무질겨서 하나도 못먹었어요.국물도 다른날이랑 달랐어요.몇젓갈안먹고 그냥왔어요.\n",
      "Summary: \n",
      "Original: ㆍ드렁킨타이 (태국음식전문)ㆍ소고기쌀국수  10,500팟타이ㆍ똠양꿍ㆍ쏨땀 등등\n",
      "Summary: \n",
      "Original: 와~~맛있고, 양이 넉넉합니다.테이블이 많지 않지만 타이밍 잘 맞으면 대기 없이 먹을 수 있어요.\n",
      "Summary: \n",
      "Original: 친구동네 갔다 식사하러 들렀는데 완전 맛집이었네요. 고소한 팟타이 와 시원한 쌀국수가 취향저격이네요\n",
      "Summary: .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 파인튜닝이 끝난 후 모델 저장\n",
    "model.save_pretrained(\"./fine_tuned_t5\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_t5\")\n",
    "\n",
    "# 요약문 생성 함수\n",
    "def generate_summary(review):\n",
    "    input_ids = tokenizer.encode(f\"summarize: {review}\", return_tensors=\"pt\").to(device)\n",
    "    summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 생성된 요약 확인\n",
    "for i in range(5):  # 5개 리뷰만 샘플로 확인\n",
    "    print(f\"Original: {reducedData['content'].iloc[i]}\")\n",
    "    print(f\"Summary: {generate_summary(reducedData['content'].iloc[i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G-01\\AppData\\Local\\Temp\\ipykernel_2876\\813604558.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reducedData['content'] = reducedData['content'].fillna('')\n",
      "C:\\Users\\G-01\\AppData\\Local\\Temp\\ipykernel_2876\\813604558.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reducedData['clean_content'] = reducedData['content'].apply(clean_text)\n",
      "C:\\Users\\G-01\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [44:05<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 0.5132179280991356\n",
      "Summaries saved to ./summarized_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G-01\\AppData\\Local\\Temp\\ipykernel_2876\\813604558.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reducedData['summary'] = summaries\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# GPU 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# T5 모델과 토크나이저 로드\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# 리뷰 데이터를 읽어옴\n",
    "file_path = '../외국음식전문점_테스트.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터의 첫 1000개 행만 사용\n",
    "reducedData = data.iloc[:3000, :]\n",
    "\n",
    "# 결측치 또는 NaN 데이터를 처리하여, NaN 값을 빈 문자열로 대체\n",
    "reducedData['content'] = reducedData['content'].fillna('')\n",
    "\n",
    "# 데이터 전처리: 불필요한 공백, 특수문자 제거\n",
    "def clean_text(text):\n",
    "    # 불필요한 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r'[^가-힣a-zA-Z., ]', '', text)\n",
    "    return text\n",
    "\n",
    "# 모든 리뷰 텍스트 전처리\n",
    "reducedData['clean_content'] = reducedData['content'].apply(clean_text)\n",
    "\n",
    "# 데이터셋 준비\n",
    "def preprocess_data(texts, max_length=512):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# 마스킹 함수\n",
    "def create_mask(input_ids):\n",
    "    return (input_ids != tokenizer.pad_token_id).float()\n",
    "\n",
    "# 전처리된 데이터로부터 입력과 마스크 생성\n",
    "train_inputs, train_masks = preprocess_data(reducedData['clean_content'])\n",
    "train_labels, _ = preprocess_data(reducedData['content'])\n",
    "\n",
    "# 배치 처리 및 데이터셋 준비\n",
    "batch_size = 4  # 배치 크기 설정\n",
    "accumulation_steps = 4  # 그래디언트 축적 단계\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 혼합 정밀도 학습을 위한 스케일러 설정\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 1  # 에포크 수 설정\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch_inputs, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        with autocast():  # 혼합 정밀도 학습 사용\n",
    "            outputs = model(input_ids=batch_inputs, attention_mask=batch_masks, labels=batch_labels)\n",
    "            loss = outputs.loss / accumulation_steps  # 그래디언트 축적을 위해 손실 나누기\n",
    "\n",
    "        scaler.scale(loss).backward()  # 스케일된 그래디언트 적용\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)  # 옵티마이저 업데이트\n",
    "            scaler.update()  # 스케일러 업데이트\n",
    "            optimizer.zero_grad()  # 그래디언트 초기화\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Loss after epoch {epoch + 1}: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# 파인튜닝이 끝난 후 모델 저장\n",
    "model.save_pretrained(\"./fine_tuned_t5\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_t5\")\n",
    "\n",
    "# 요약문 생성 함수\n",
    "def generate_summary(review):\n",
    "    input_ids = tokenizer.encode(f\"summarize: {review}\", return_tensors=\"pt\").to(device)\n",
    "    summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 요약문 생성 및 CSV 파일 저장\n",
    "summaries = []\n",
    "for review in reducedData['content']:\n",
    "    summary = generate_summary(review)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# 요약문을 데이터프레임에 추가\n",
    "reducedData['summary'] = summaries\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_file = './summarized_reviews.csv'\n",
    "reducedData.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Summaries saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G-01\\AppData\\Local\\Temp\\ipykernel_2876\\1211206812.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reducedData['content'] = reducedData['content'].fillna('')\n",
      "C:\\Users\\G-01\\AppData\\Local\\Temp\\ipykernel_2876\\1211206812.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reducedData['clean_content'] = reducedData['content'].apply(clean_text)\n",
      "C:\\Users\\G-01\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [1:14:39<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 0.3356435199826956\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [1:09:58<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 2: 0.02205612166188657\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [1:03:45<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 3: 0.01386244243402034\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [1:00:23<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 4: 0.010958180238679052\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [49:56<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 5: 0.009299165822472423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1636 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store summaries saved to ./summarized_reviews_by_store.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# GPU 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# T5 모델과 토크나이저 로드\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# 리뷰 데이터를 읽어옴\n",
    "file_path = '../외국음식전문점.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터의 첫 1000개 행만 사용\n",
    "reducedData = data.iloc[:5000, :]\n",
    "\n",
    "# 결측치 또는 NaN 데이터를 처리하여, NaN 값을 빈 문자열로 대체\n",
    "reducedData['content'] = reducedData['content'].fillna('')\n",
    "\n",
    "# 데이터 전처리: 불필요한 공백, 특수문자 제거\n",
    "def clean_text(text):\n",
    "    # 불필요한 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r'[^가-힣a-zA-Z., ]', '', text)\n",
    "    return text\n",
    "\n",
    "# 모든 리뷰 텍스트 전처리\n",
    "reducedData['clean_content'] = reducedData['content'].apply(clean_text)\n",
    "\n",
    "# 데이터셋 준비\n",
    "def preprocess_data(texts, max_length=512):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# 마스킹 함수\n",
    "def create_mask(input_ids):\n",
    "    return (input_ids != tokenizer.pad_token_id).float()\n",
    "\n",
    "# 전처리된 데이터로부터 입력과 마스크 생성\n",
    "train_inputs, train_masks = preprocess_data(reducedData['clean_content'])\n",
    "train_labels, _ = preprocess_data(reducedData['content'])\n",
    "\n",
    "# 배치 처리 및 데이터셋 준비\n",
    "batch_size = 4  # 배치 크기 설정\n",
    "accumulation_steps = 4  # 그래디언트 축적 단계\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 혼합 정밀도 학습을 위한 스케일러 설정\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 5  # 에포크 수 설정\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch_inputs, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        with autocast():  # 혼합 정밀도 학습 사용\n",
    "            outputs = model(input_ids=batch_inputs, attention_mask=batch_masks, labels=batch_labels)\n",
    "            loss = outputs.loss / accumulation_steps  # 그래디언트 축적을 위해 손실 나누기\n",
    "\n",
    "        scaler.scale(loss).backward()  # 스케일된 그래디언트 적용\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)  # 옵티마이저 업데이트\n",
    "            scaler.update()  # 스케일러 업데이트\n",
    "            optimizer.zero_grad()  # 그래디언트 초기화\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Loss after epoch {epoch + 1}: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# 파인튜닝이 끝난 후 모델 저장\n",
    "model.save_pretrained(\"./fine_tuned_t5\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_t5\")\n",
    "\n",
    "# 가게별로 리뷰를 묶는 함수\n",
    "def group_reviews_by_store(data, store_col='store_name', review_col='content'):\n",
    "    # 가게별 리뷰를 모두 합쳐서 하나의 큰 텍스트로 만듦\n",
    "    grouped_data = data.groupby(store_col)[review_col].apply(lambda x: ' '.join(x)).reset_index()\n",
    "    return grouped_data\n",
    "\n",
    "# 가게별 리뷰를 묶어서 데이터를 준비\n",
    "grouped_reviews = group_reviews_by_store(reducedData)\n",
    "\n",
    "# 요약문 생성 함수\n",
    "def generate_summary_for_store(store_reviews):\n",
    "    input_ids = tokenizer.encode(f\"summarize: {store_reviews}\", return_tensors=\"pt\").to(device)\n",
    "    summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 가게별 요약문 생성\n",
    "summaries = []\n",
    "for store_reviews in grouped_reviews['content']:\n",
    "    summary = generate_summary_for_store(store_reviews)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# 요약문을 데이터프레임에 추가\n",
    "grouped_reviews['summary'] = summaries\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_file = './summarized_reviews_by_store.csv'\n",
    "grouped_reviews.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Store summaries saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
