{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import urllib.request\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".--- from numpy import dot\n",
    "의도 및 역할:\n",
    "dot 함수는 두 배열의 내적(dot product)을 계산하는 데 사용됩니다. 벡터와 행렬의 곱셈을 수행할 때 자주 사용됩니다.\n",
    "from numpy import dot는 numpy 라이브러리에서 dot 함수를 직접 가져와 코드에서 np.dot 대신 dot으로 간편하게 사용할 수 있도록 합니다.\n",
    "사용 예시:\n",
    "\n",
    "두 벡터 간의 내적을 계산하여 코사인 유사도를 구하는 데 사용될 수 있습니다.\n",
    "예를 들어, 두 벡터 A와 B에 대해 dot(A, B)를 사용하여 그들의 내적을 계산할 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    ".--- from numpy.linalg import norm\n",
    "의도 및 역할:\n",
    "\n",
    "norm 함수는 벡터나 행렬의 노름(norm)을 계산하는 데 사용됩니다. 노름은 벡터의 크기 또는 길이를 측정하는 방법으로, 다양한 종류의 노름(L1, L2 등)을 계산할 수 있습니다.\n",
    "from numpy.linalg import norm는 numpy.linalg 모듈에서 norm 함수를 직접 가져와 코드에서 np.linalg.norm 대신 norm으로 간편하게 사용할 수 있도록 합니다.\n",
    "사용 예시:\n",
    "\n",
    "벡터의 크기(유클리드 거리 등)를 계산하는 데 사용됩니다.\n",
    "예를 들어, 벡터 A의 L2 노름(유클리드 거리)을 계산하려면 norm(A)를 사용할 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    ".--- import urllib.request\n",
    "의도 및 역할:\n",
    "\n",
    "urllib.request는 Python 표준 라이브러리의 일부로, 웹에서 데이터를 가져오거나 파일을 다운로드할 때 사용됩니다. HTTP(S) 요청을 통해 웹 리소스에 접근할 수 있습니다.\n",
    "import urllib.request는 이 모듈을 임포트하여 코드에서 URL을 통해 데이터를 다운로드하거나 웹 리소스에 접근할 수 있도록 합니다.\n",
    "사용 예시:\n",
    "\n",
    "URL을 통해 데이터를 다운로드하고 이를 파일로 저장하는 데 사용됩니다.\n",
    "예를 들어, CSV 파일을 웹에서 다운로드하여 로컬에 저장하려면 urllib.request.urlretrieve(url, filename)을 사용할 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    ".--- from sentence_transformers import SentenceTransformer\n",
    "의도 및 역할:\n",
    "\n",
    "SentenceTransformer는 Sentence-Transformers 라이브러리에서 제공하는 클래스입니다. 이 라이브러리는 BERT, RoBERTa, DistilBERT 등 다양한 트랜스포머 모델을 기반으로 문장의 임베딩(고차원 벡터 표현)을 생성하는 기능을 제공합니다.\n",
    "from sentence_transformers import SentenceTransformer는 이 클래스를 임포트하여 코드에서 자연어 문장의 임베딩을 생성하고, 이를 다양한 NLP 작업에 사용할 수 있도록 합니다.\n",
    "사용 예시:\n",
    "\n",
    "문장을 고차원 벡터로 변환하여 텍스트 유사도 계산, 군집화, 분류 등의 작업에 사용됩니다.\n",
    "예를 들어, model = SentenceTransformer('model_name')를 통해 특정 트랜스포머 모델을 로드하고, model.encode(sentence)를 통해 문장을 벡터로 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n",
    "train_data = pd.read_csv('ChatBotData.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    ".---urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n",
    "\n",
    "이 코드 줄은 urllib.request 라이브러리를 사용하여 지정된 URL에서 CSV 파일을 다운로드하고, 로컬 파일 시스템에 저장합니다.\n",
    "urlretrieve 함수는 두 개의 인수를 받습니다:\n",
    "첫 번째 인수는 다운로드할 파일의 URL입니다.\n",
    "두 번째 인수는 로컬에 저장할 파일의 경로 및 이름입니다.\n",
    "이 예제에서는 \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\"에서 ChatBotData.csv 파일을 다운로드하고, 이를 로컬 디렉토리에 ChatBotData.csv라는 이름으로 저장합니다.\n",
    "------------------------------------------------------------------------------------------------------------0--\n",
    "사용된 라이브러리 (urllib.request):\n",
    "\n",
    "urllib.request는 웹에서 데이터를 다운로드하거나 HTTP 요청을 보내는 데 사용됩니다.\n",
    "여기서는 urlretrieve를 사용하여 웹에서 CSV 파일을 다운로드합니다.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------0--\n",
    ".---train_data = pd.read_csv('ChatBotData.csv')\n",
    "\n",
    "이 코드 줄은 로컬에 저장된 ChatBotData.csv 파일을 읽어들여, 이를 pandas 데이터프레임으로 변환합니다.\n",
    "pd.read_csv() 함수는 CSV 파일을 읽어들여 이를 구조화된 데이터 형태인 데이터프레임으로 변환합니다.\n",
    "변환된 데이터프레임은 train_data 변수에 저장됩니다. 이 데이터프레임은 이후 데이터 분석 및 처리 작업에 사용됩니다.\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------0--\n",
    ".---train_data.head()\n",
    "\n",
    "이 코드 줄은 train_data 데이터프레임의 첫 몇 개의 행을 출력합니다. 기본적으로 head() 함수는 처음 5개의 행을 반환합니다.\n",
    "이를 통해 데이터가 올바르게 로드되었는지, 데이터의 구조는 어떤지 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentenceTransformer는 sentence-transformers 라이브러리에서 제공하는 클래스입니다. 이 라이브러리는 다양한 트랜스포머(Transformer) 모델을 사용하여 문장의 임베딩(고차원 벡터 표현)을 생성하는 데 특화되어 있습니다.\n",
    "이 클래스는 주로 텍스트 간의 유사도 계산, 검색, 클러스터링, 분류 등과 같은 NLP(Natural Language Processing) 작업에 사용됩니다.\n",
    "SentenceTransformer 인스턴스를 생성할 때, 특정 사전 훈련된 모델을 불러와서 이후 자연어 처리 작업에 활용할 수 있게 됩니다.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------0--\n",
    "\n",
    "'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens'\n",
    "\n",
    "이 문자열은 불러올 특정 모델을 지정하는 역할을 합니다. 여기서 사용된 모델은 'xlm-r-100langs-bert-base-nli-stsb-mean-tokens'입니다.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------0--\n",
    "xlm-r-100langs-bert-base:\n",
    "XLM-R은 다국어로 학습된 BERT 모델로, 100개의 언어를 지원합니다. 이 모델은 특히 다양한 언어로 문장을 벡터화하는 작업에 적합합니다.\n",
    "------------------------------------------------------------------------------------------------------------0--\n",
    "nli-stsb:\n",
    "이 모델은 먼저 자연어 추론(Natural Language Inference, NLI) 데이터셋으로 훈련된 후, STS-B(Stanford Natural Language Inference Benchmark) 데이터셋으로 추가 미세 조정(Fine-Tuning)된 모델입니다.\n",
    "NLI와 STS-B 데이터셋을 사용해 학습한 모델은 문장 간의 유사성을 효과적으로 계산할 수 있습니다.\n",
    "------------------------------------------------------------------------------------------------------------0--\n",
    "mean-tokens:\n",
    "문장의 각 단어 토큰의 임베딩을 평균(mean pooling)하여 최종 문장 임베딩을 생성합니다. 이 방식은 문장 전체를 하나의 벡터로 요약하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 질문에 대한 임베딩을 배치로 생성\n",
    "embeddings = model.encode(train_data['Q'].tolist(), batch_size=32)\n",
    "\n",
    "# 임베딩을 리스트로 변환하여 데이터프레임에 추가\n",
    "train_data['embedding'] = list(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings = model.encode(train_data['Q'].tolist(), batch_size=32)\n",
    "\n",
    "이 줄의 코드는 train_data 데이터프레임의 'Q' 열에 있는 모든 질문들을 문장 임베딩으로 변환하고, 그 결과를 embeddings 변수에 저장합니다. 이 과정은 배치(batch) 처리를 통해 효율적으로 수행됩니다.\n",
    "\n",
    "세부 설명:\n",
    "\n",
    "train_data['Q']:\n",
    "\n",
    "train_data는 이전에 로드된 데이터프레임으로, 일반적으로 질문과 답변 쌍을 포함하는 데이터셋입니다.\n",
    "'Q'는 데이터프레임의 열(column) 이름으로, 여기서는 각 행(row)에 해당하는 질문들이 포함되어 있습니다.\n",
    ".tolist():\n",
    "\n",
    "pandas 시리즈인 train_data['Q']를 Python의 리스트(list) 형식으로 변환합니다.\n",
    "이는 SentenceTransformer 모델의 encode 메서드가 리스트 형태의 입력을 받기 때문에 필요합니다.\n",
    "예를 들어, ['안녕하세요', '오늘 날씨 어때요?', '너의 이름은 뭐니?'] 와 같은 형태의 리스트가 생성됩니다.\n",
    "model.encode(…, batch_size=32):\n",
    "\n",
    "model은 이전에 로드한 SentenceTransformer 모델의 인스턴스입니다.\n",
    "encode 메서드는 입력된 문장 또는 문장들의 리스트를 고차원 벡터인 임베딩으로 변환합니다.\n",
    "batch_size=32:\n",
    "배치 크기를 32로 설정하여 한 번에 32개의 문장을 처리합니다.\n",
    "배치 처리는 대량의 데이터를 처리할 때 메모리 효율성과 처리 속도를 향상시키기 위한 방법입니다.\n",
    "배치 크기는 시스템의 메모리 용량과 성능에 따라 조정할 수 있습니다.\n",
    "\n",
    "동작 과정:\n",
    "입력된 문장 리스트를 배치 크기만큼 분할합니다.\n",
    "각 배치를 순차적으로 모델에 입력하여 임베딩을 생성합니다.\n",
    "모든 배치의 결과를 합쳐서 최종 임베딩 배열을 생성합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embeddings 변수에는 각 문장에 해당하는 임베딩 벡터들이 numpy 배열 형태로 저장됩니다.\n",
    "만약 질문이 1,000개라면, embeddings의 형태는 (1000, 768)이 됩니다. 여기서 768은 사용된 모델의 임베딩 차원입니다\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------\n",
    "사용된 라이브러리와 메서드의 역할:\n",
    "\n",
    "SentenceTransformer 라이브러리:\n",
    "\n",
    "자연어 문장을 고차원 벡터 공간으로 변환하여 기계가 이해할 수 있는 형태로 만드는 데 사용됩니다.\n",
    "이러한 임베딩은 문장 간 유사도 계산, 분류, 군집화 등 다양한 NLP 작업에 활용됩니다.\n",
    "encode 메서드:\n",
    "\n",
    "입력된 문장 또는 문장 리스트를 처리하여 각 문장에 대한 고정된 크기의 벡터 표현을 생성합니다.\n",
    "내부적으로 토큰화(tokenization), 모델 전방향 연산(forward pass), 풀링(pooling) 등의 과정을 거칩니다.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------\n",
    "train_data['embedding'] = list(embeddings)\n",
    "의도 및 역할:\n",
    "생성된 임베딩 배열을 리스트로 변환하여 train_data 데이터프레임의 새로운 열인 'embedding'에 추가합니다. 이를 통해 각 질문과 해당 임베딩이 같은 행에 위치하게 되어 이후의 분석 및 처리 작업이 용이해집니다.\n",
    "\n",
    "세부 설명:\n",
    "\n",
    "list(embeddings):\n",
    "embeddings는 numpy 배열 형태로 되어 있습니다.\n",
    "이를 Python의 리스트 형태로 변환합니다.\n",
    "리스트로 변환하는 이유는 pandas 데이터프레임이 각 셀에 복잡한 객체를 저장할 때 리스트 형태가 더 적합하기 때문입니다.\n",
    "각 요소는 개별 문장의 임베딩 벡터(numpy 배열)입니다.\n",
    "train_data['embedding'] = …:\n",
    "train_data 데이터프레임에 새로운 열 'embedding'을 생성하고, 앞서 변환한 임베딩 리스트를 할당합니다.\n",
    "결과적으로 데이터프레임의 각 행에는 다음과 같은 구조가 됩니다:\n",
    "'Q': 질문 문장 (예: \"안녕하세요\")\n",
    "'A': 해당 질문에 대한 답변 (예: \"안녕하세요. 무엇을 도와드릴까요?\")\n",
    "'embedding': 질문 문장의 임베딩 벡터 (예: [0.1234, -0.5678, ..., 0.9101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentenceTransformer 객체의 정보가 출력됩니다. 이 출력은 모델의 구조, 사용된 모듈, 그리고 해당 모델의 구성 요소에 대한 요약 정보를 제공합니다. 여기서 model은 SentenceTransformer 클래스의 인스턴스이며, 이전에 정의된 'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens' 모델을 사용하고 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n"
     ]
    }
   ],
   "source": [
    "print(model._first_module())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentenceTransformer 객체의 첫 번째 모듈에 대한 정보를 출력하는 코드입니다. SentenceTransformer 모델은 여러 모듈로 구성되며, 일반적으로 첫 번째 모듈은 트랜스포머 모델(예: BERT, RoBERTa, XLM-R)입니다. 이 코드 줄을 통해 이 첫 번째 모듈의 구조와 세부 정보를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 278043648\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentenceTransformer 모델에 포함된 전체 파라미터의 수를 계산하고, 그 값을 출력하는 역할을 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0.auto_model.embeddings.word_embeddings.weight | Size: torch.Size([250002, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.embeddings.position_embeddings.weight | Size: torch.Size([514, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.embeddings.token_type_embeddings.weight | Size: torch.Size([1, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.embeddings.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.embeddings.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.0.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.1.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.2.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.3.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.4.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.5.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.6.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.7.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.8.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.9.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.10.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.encoder.layer.11.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.pooler.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: 0.auto_model.pooler.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드 블럭은 SentenceTransformer 모델의 모든 파라미터를 순회하면서 각 레이어의 이름, 파라미터의 크기, 그리고 해당 파라미터가 학습 가능한지(즉, 그래디언트가 필요한지)를 출력하는 역할을 합니다. 이 정보를 통해 모델의 내부 구조와 각 레이어의 파라미터에 대한 상세한 정보를 확인할 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.for name, param in model.named_parameters():\n",
    "\n",
    "이 줄은 SentenceTransformer 모델의 모든 파라미터들을 순회(iterate)하면서, 각 파라미터의 이름과 해당 파라미터 텐서를 가져옵니다.\n",
    "model.named_parameters()는 모델의 모든 파라미터를 (name, parameter) 쌍의 형태로 반환합니다. 여기서 name은 파라미터가 속한 레이어의 이름, parameter는 실제 파라미터 텐서를 의미합니다.\n",
    "세부 설명:\n",
    "\n",
    "model.named_parameters():\n",
    "\n",
    "이 메서드는 모델의 모든 파라미터를 반환할 뿐만 아니라, 해당 파라미터가 속한 레이어의 이름도 함께 반환합니다.\n",
    "각 레이어의 파라미터가 (name, param) 쌍으로 반환되며, name은 문자열로 된 레이어 이름이고, param은 torch.Tensor 객체로, 해당 레이어의 가중치나 편향 등의 파라미터를 나타냅니다.\n",
    "for name, param in …:\n",
    "\n",
    "for 루프는 model.named_parameters()에서 반환된 (name, param) 쌍들을 하나씩 순회하면서 name과 param에 각각 할당합니다.\n",
    "이 루프는 모델의 모든 레이어와 그 파라미터에 대해 한 번씩 반복됩니다.\n",
    "\n",
    "\n",
    "2. print(f\"Layer: {name} | Size: {param.size()} | Requires Grad: {param.requires_grad}\")\n",
    "\n",
    "이 줄은 각 레이어의 이름, 파라미터의 크기, 그리고 해당 파라미터가 학습 가능(gradient 필요) 여부를 출력합니다.\n",
    "이를 통해 모델의 각 레이어가 어떤 파라미터를 가지고 있고, 그 파라미터의 형태와 학습 가능 상태를 한눈에 파악할 수 있습니다.\n",
    "세부 설명:\n",
    "\n",
    "f\"Layer: {name} | Size: {param.size()} | Requires Grad: {param.requires_grad}\":\n",
    "f 문자열은 포맷된 문자열 리터럴로, 중괄호 {} 안에 변수나 표현식을 삽입할 수 있습니다.\n",
    "{name}: 레이어의 이름을 출력합니다.\n",
    "{param.size()}: 파라미터 텐서의 크기(shape)를 출력합니다. 이는 텐서의 차원과 각 차원에서의 요소 수를 나타냅니다.\n",
    "예를 들어, (768, 3072)는 768개의 입력 피처와 3072개의 출력 피처를 가진 파라미터를 의미합니다.\n",
    "{param.requires_grad}: 파라미터가 학습 가능한지 여부를 Boolean 값(True 또는 False)으로 출력합니다.\n",
    "True이면, 학습 중에 이 파라미터가 업데이트됩니다.\n",
    "False이면, 이 파라미터는 고정되어 있으며 학습 중에 업데이트되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(A, B):\n",
    "  return dot(A, B)/(norm(A)*norm(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수는 두 벡터 \n",
    "A와 B 사이의 코사인 유사도(Cosine Similarity)를 계산하는 역할을 합니다. 코사인 유사도는 두 벡터 간의 방향성 유사성을 측정하는 방법으로, 두 벡터가 얼마나 유사한지, 즉 동일한 방향을 가리키는지를 나타냅니다. 코사인 유사도는 정보 검색, 추천 시스템, 텍스트 분석 등 다양한 분야에서 벡터 간의 유사성을 측정할 때 자주 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_answer(question):\n",
    "    embedding = model.encode(question)\n",
    "    train_data['score'] = train_data.apply(lambda x: cos_sim(x['embedding'], embedding), axis=1)\n",
    "    return train_data.loc[train_data['score'].idxmax()]['A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. embedding = model.encode(question)\n",
    "\n",
    "입력된 question(질문)을 문장 임베딩(고차원 벡터)으로 변환합니다.\n",
    "model.encode(question)은 SentenceTransformer 모델을 사용해 주어진 질문을 벡터로 변환하는 과정입니다.\n",
    "세부 설명:\n",
    "\n",
    "model: 이전에 정의된 SentenceTransformer 모델입니다. 이 모델은 문장을 벡터로 변환하는 역할을 합니다.\n",
    "encode(question): question 문장을 입력으로 받아, 그 문장을 고차원 벡터로 변환합니다. 이 벡터는 문장의 의미를 수치적으로 표현한 것입니다.\n",
    "embedding: 결과로 생성된 벡터로, 이 벡터는 이후 데이터셋의 다른 문장 벡터와 유사도를 비교하는 데 사용됩니다.\n",
    "\n",
    "\n",
    "2. train_data['score'] = train_data.apply(lambda x: cos_sim(x['embedding'], embedding), axis=1)\n",
    "\n",
    "데이터셋 train_data 내의 모든 질문(이미 임베딩된 질문들)과 입력된 질문의 임베딩 간의 코사인 유사도를 계산하고, 그 결과를 score 열에 저장합니다.\n",
    "세부 설명:\n",
    "\n",
    "train_data.apply(lambda x: ..., axis=1): 데이터프레임의 각 행에 대해 함수를 적용합니다. 여기서는 cos_sim 함수를 적용하여 유사도를 계산합니다.\n",
    "lambda x: cos_sim(x['embedding'], embedding): 각 행에 대해 cos_sim 함수를 호출하는 람다 함수입니다.\n",
    "x['embedding']: train_data 데이터프레임의 각 행에서 사전 계산된 질문의 임베딩을 가져옵니다.\n",
    "embedding: 현재 함수의 입력으로 주어진 질문에 대한 임베딩입니다.\n",
    "cos_sim: 두 벡터 간의 코사인 유사도를 계산하는 함수입니다.\n",
    "train_data['score']: 각 행에 대해 계산된 코사인 유사도 값을 새로운 score 열에 저장합니다. 이 열은 입력된 질문과 데이터셋 내 각 질문의 유사도를 나타냅니다.\n",
    "\n",
    "\n",
    "3. return train_data.loc[train_data['score'].idxmax()]['A']\n",
    "\n",
    "가장 높은 유사도(score)를 가진 질문의 답변을 반환합니다.\n",
    "세부 설명:\n",
    "\n",
    "train_data['score'].idxmax(): score 열에서 가장 큰 값을 가지는 행의 인덱스를 반환합니다. 즉, 입력된 질문과 가장 유사한 질문이 위치한 행의 인덱스를 찾습니다.\n",
    "train_data.loc[...]: 위에서 찾은 인덱스에 해당하는 데이터프레임의 행을 선택합니다.\n",
    "['A']: 해당 행의 A 열(답변)을 선택합니다.\n",
    "return: 선택된 답변을 반환합니다. 이는 입력된 질문과 가장 유사한 질문에 해당하는 답변입니다.\n",
    "\n",
    "\n",
    "\n",
    "함수의 전체적인 동작\n",
    "문장 임베딩 생성: 입력된 질문을 벡터화합니다.\n",
    "코사인 유사도 계산: 데이터셋 내의 모든 질문과 입력된 질문 간의 유사도를 계산합니다.\n",
    "가장 유사한 질문의 답변 반환: 가장 유사한 질문을 찾고, 그 질문에 대한 답변을 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주의사항\n",
    "\n",
    "\n",
    "데이터셋 train_data의 구조: train_data 데이터프레임은 질문(Q)과 답변(A) 쌍으로 구성되어 있으며, 각 질문의 임베딩이 이미 계산되어 embedding 열에 저장되어 있어야 합니다.\n",
    "질문의 다양성: 입력된 질문이 데이터셋 내의 질문들과 많이 다르다면, 반환된 답변이 적절하지 않을 수 있습니다. 이는 데이터셋의 범위와 품질에 따라 달라집니다.\n",
    "유사도 계산의 정확성: 코사인 유사도는 벡터 간의 방향성을 비교하는 방법이므로, 질문의 의미가 다르더라도 유사도가 높게 나올 수 있습니다. 이에 따라 반환된 답변이 항상 정확하지 않을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'카페인이 필요한 시간인가 봐요.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_answer('나랑 커피먹을래?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'얼른 맛난 음식 드세요.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_answer('배고프다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'산책 좀 해야겠네여.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_answer('빠큐')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
