{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53319 entries, 0 to 53318\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   store_name  53319 non-null  object\n",
      " 1   content     51124 non-null  object\n",
      " 2   date        53314 non-null  object\n",
      " 3   revisit     53314 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "       store_name content    date revisit\n",
      "count       53319   51124   53314   53314\n",
      "unique        737   42580    1527      88\n",
      "top       타이인플레이트       굿  8.17.토  1번째 방문\n",
      "freq          200    2092     388   42863\n",
      "  store_name                                            content    date  \\\n",
      "0  드렁킨타이 목동점                         언제나줄서야먹을수있는테이블좀 늘려주세요 아님확장  7.19.금   \n",
      "1  드렁킨타이 목동점  목동에서 젤좋아하는 곳인데 오늘 고기도 여러점인데도  너무질겨서 하나도 못먹었어요....   7.4.목   \n",
      "2  드렁킨타이 목동점        ㆍ드렁킨타이 (태국음식전문)ㆍ소고기쌀국수  10,500팟타이ㆍ똠양꿍ㆍ쏨땀 등등  6.20.목   \n",
      "3  드렁킨타이 목동점  와~~맛있고, 양이 넉넉합니다.테이블이 많지 않지만 타이밍 잘 맞으면 대기 없이 먹...  6.20.목   \n",
      "4  드렁킨타이 목동점  친구동네 갔다 식사하러 들렀는데 완전 맛집이었네요. 고소한 팟타이 와 시원한 쌀국수...  6.14.금   \n",
      "\n",
      "  revisit  \n",
      "0  1번째 방문  \n",
      "1  1번째 방문  \n",
      "2  1번째 방문  \n",
      "3  1번째 방문  \n",
      "4  1번째 방문  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = './외국음식전문점.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터프레임의 기본 정보 확인\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Install MeCab in order to use it: http://konlpy.org/en/latest/install/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\konlpy\\tag\\_mecab.py:77\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagger \u001b[38;5;241m=\u001b[39m \u001b[43mTagger\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-d \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dicpath)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagset \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/data/tagset/mecab.json\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m utils\u001b[38;5;241m.\u001b[39minstallpath)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tagger' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkonlpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mecab\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Mecab 형태소 분석기 사용 (필요시 다른 분석기 사용 가능)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m mecab \u001b[38;5;241m=\u001b[39m \u001b[43mMecab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_preprocessing\u001b[39m(text):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# 특수문자 제거\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^ㄱ-ㅎㅏ-ㅣ가-힣\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\konlpy\\tag\\_mecab.py:82\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe MeCab dictionary does not exist at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Is the dictionary correctly installed?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou can also try entering the dictionary path when initializing the Mecab class: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMecab(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m/some/dic/path\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dicpath)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstall MeCab in order to use it: http://konlpy.org/en/latest/install/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Install MeCab in order to use it: http://konlpy.org/en/latest/install/"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Mecab 형태소 분석기 사용 (필요시 다른 분석기 사용 가능)\n",
    "mecab = Mecab()\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣\\s]\", \"\", text)\n",
    "    # 형태소 분석\n",
    "    tokens = mecab.morphs(text)\n",
    "    # 토큰을 다시 문자열로 결합\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# 텍스트 열에 전처리 적용 (예: 'text_column'이 텍스트 데이터가 있는 열이라고 가정)\n",
    "data['processed_text'] = data['text_column'].apply(text_preprocessing)\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(data['processed_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치 및 임포트\n",
    "# pip install torch transformers git+https://github.com/SKT-AI/KoBART#egg=kobart rouge\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from rouge import Rouge\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "# CSV 파일을 로드합니다. 이 단계에서는 데이터가 이미 전처리된 상태라고 가정합니다.\n",
    "data = pd.read_csv('./외국음식전문점.csv')  # 이전에 전처리된 데이터를 저장한 파일\n",
    "texts = data['processed_text'].tolist()  # 전처리된 텍스트 열을 리스트로 변환\n",
    "\n",
    "# 2. KoBART 모델 및 토크나이저 로드\n",
    "# KoBART는 한국어 문서 요약에 특화된 BART 모델입니다.\n",
    "# 장점: 한국어에 최적화된 모델로, 문맥을 이해하여 자연스러운 요약을 생성할 수 있습니다.\n",
    "# 단점: 학습에 많은 자원이 필요하며, 장문을 요약할 때 성능이 제한적일 수 있습니다.\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v1')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
    "\n",
    "# 3. 데이터셋 토큰화\n",
    "# 텍스트 데이터를 모델에 입력할 수 있도록 토큰화합니다.\n",
    "inputs = tokenizer(texts, max_length=512, truncation=True, return_tensors='pt', padding=True)\n",
    "\n",
    "# 예시로 요약할 문장을 동일하게 생성합니다.\n",
    "summary_texts = [\"이 문서는 요약을 필요로 합니다.\"] * len(texts)\n",
    "labels = tokenizer(summary_texts, max_length=128, truncation=True, return_tensors='pt', padding=True).input_ids\n",
    "\n",
    "# 4. 데이터셋 및 데이터로더 구성\n",
    "# 데이터셋을 구성하고, 모델 학습을 위한 데이터로더를 만듭니다.\n",
    "# 장점: DataLoader를 통해 배치 학습을 수행할 수 있으며, GPU 메모리 사용을 효율화할 수 있습니다.\n",
    "# 단점: 데이터가 너무 크면 메모리 부족이 발생할 수 있습니다.\n",
    "dataset = TensorDataset(inputs.input_ids, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 5. 옵티마이저 설정\n",
    "# AdamW 옵티마이저를 사용하여 모델을 학습합니다.\n",
    "# 장점: AdamW는 일반적인 Adam보다 L2 정규화가 추가되어 과적합을 줄이는 데 도움을 줍니다.\n",
    "# 단점: 학습률 설정이 민감하여 적절한 값을 찾는 데 시간이 걸릴 수 있습니다.\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 6. 모델 학습\n",
    "# 모델을 학습시키는 과정입니다. 에포크 수를 설정하고, 각 배치마다 손실을 계산하여 역전파합니다.\n",
    "# 장점: 모델이 학습을 통해 점점 더 나은 예측을 할 수 있게 됩니다.\n",
    "# 단점: 학습 시간이 오래 걸릴 수 있으며, 특히 대규모 데이터셋의 경우 GPU 자원이 필수적입니다.\n",
    "model.train()\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        input_ids, labels = batch\n",
    "        optimizer.zero_grad()  # 이전 배치의 gradients를 초기화합니다.\n",
    "        outputs = model(input_ids=input_ids, labels=labels)  # 모델에 입력하여 출력과 손실을 계산합니다.\n",
    "        loss = outputs.loss  # 손실값 추출\n",
    "        loss.backward()  # 역전파를 통해 모델 파라미터를 업데이트합니다.\n",
    "        optimizer.step()  # 옵티마이저를 사용하여 모델 파라미터를 한 단계 업데이트합니다.\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')  # 현재 에포크와 손실값 출력\n",
    "\n",
    "# 7. 모델 평가\n",
    "# 학습이 완료된 모델을 평가 모드로 전환한 후, 새로운 데이터를 입력하여 요약 문장을 생성합니다.\n",
    "# 장점: 평가 모드에서는 dropout 등 학습 시 사용되던 불확실성을 제거하여 안정적인 예측을 제공합니다.\n",
    "# 단점: 생성된 요약 문장의 품질이 데이터셋의 특성에 크게 좌우될 수 있습니다.\n",
    "model.eval()\n",
    "\n",
    "# 새로운 데이터 예시로 요약 생성\n",
    "new_texts = [\"이것은 테스트를 위한 새로운 문장입니다.\"]\n",
    "new_inputs = tokenizer(new_texts, max_length=512, truncation=True, return_tensors='pt', padding=True)\n",
    "\n",
    "# 요약 문장 생성\n",
    "with torch.no_grad():  # 평가 모드에서는 gradient 계산을 하지 않습니다.\n",
    "    summary_ids = model.generate(new_inputs.input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
    "    summaries = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "\n",
    "print(\"Generated Summaries:\", summaries)  # 생성된 요약 문장을 출력합니다.\n",
    "\n",
    "# 8. 성능 평가\n",
    "# Rouge 점수를 사용하여 모델이 생성한 요약 문장의 품질을 평가합니다.\n",
    "# 장점: Rouge 점수는 요약 문장이 원본 문장을 얼마나 잘 반영하고 있는지를 정량적으로 평가할 수 있습니다.\n",
    "# 단점: Rouge 점수는 길이가 긴 문장보다는 짧은 문장에 유리할 수 있습니다.\n",
    "rouge = Rouge()\n",
    "reference_summaries = [\"이것은 테스트를 위한 요약 문장입니다.\"]  # 참고 요약 문장\n",
    "scores = rouge.get_scores(summaries, reference_summaries)  # 생성된 요약과 참고 요약 비교\n",
    "\n",
    "print(\"ROUGE Scores:\", scores)  # Rouge 점수를 출력하여 성능을 평가합니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
