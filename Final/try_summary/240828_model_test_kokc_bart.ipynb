{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing and Tokenizing Reviews: 100%|██████████| 1000/1000 [00:00<00:00, 199652.70it/s]\n",
      "c:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model Training Completed.\n",
      "(0, '0.006*\"너무\" + 0.004*\"맛있게\" + 0.003*\"진짜\" + 0.003*\"거\"')\n",
      "(1, '0.021*\"너무\" + 0.007*\"맛있어요\" + 0.007*\"정말\" + 0.007*\"맛있고\"')\n",
      "(2, '0.007*\"너무\" + 0.007*\"좋은\" + 0.005*\"좋아요\" + 0.005*\"엄청\"')\n",
      "(3, '0.007*\"정말\" + 0.007*\"맛있고\" + 0.005*\"또\" + 0.005*\"수\"')\n",
      "(4, '0.008*\"맛있어요\" + 0.008*\"맛있게\" + 0.008*\"또\" + 0.006*\"너무\"')\n",
      "(5, '0.033*\"굿\" + 0.016*\"너무\" + 0.004*\"또\" + 0.004*\"진짜\"')\n",
      "(6, '0.030*\"좋아요\" + 0.015*\"너무\" + 0.012*\"잘\" + 0.006*\"다\"')\n",
      "(7, '0.013*\"너무\" + 0.010*\"맛있고\" + 0.008*\"진짜\" + 0.007*\"넘\"')\n",
      "(8, '0.007*\"잘\" + 0.007*\"너무\" + 0.006*\"맛있어요\" + 0.003*\"좋은\"')\n",
      "(9, '0.059*\"맛있어요\" + 0.026*\"너무\" + 0.008*\"넘\" + 0.007*\"좋고\"')\n",
      "Cluster 0 top terms: 먹어본적, 먹어본맛인데여긴, 먹어봐서, 너무, 맛있고, 맛이, 친절하고, 혼밥하기, 분위기, 좋아요\n",
      "Cluster 1 top terms: 진짜, 자주, 좋아요, 친절하시고, 분위기도, 맛있고, 음식도, 맛있어요, 좋고, 너무\n",
      "Cluster 2 top terms: 양도, 항상, 먹었습니다, 쌀국수, 정말, 좋아요, 맛있게, 진짜, 맛있고, 맛있어요\n",
      "Cluster 3 top terms: 방문했어요, 맛도, 신메뉴가, 메뉴, 계란이, 적어, 가볍게, 너무, 먹기, 좋습니다\n",
      "Cluster 4 top terms: 먹어볼래요, 힙한, 쌀국수도, 분짜, 음식도, 음식, 항상, 쌀국수, 너무, 맛있어요\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G-01\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "KoBERT Training Epoch 1/3: 100%|██████████| 63/63 [03:22<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 Loss: 0.6988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KoBERT Training Epoch 2/3: 100%|██████████| 63/63 [03:21<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 Loss: 0.6970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KoBERT Training Epoch 3/3: 100%|██████████| 63/63 [03:18<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 Loss: 0.7016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KC-BERT Training Epoch 1/3: 100%|██████████| 63/63 [04:21<00:00,  4.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 Loss: 0.7130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KC-BERT Training Epoch 2/3: 100%|██████████| 63/63 [04:16<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 Loss: 0.6739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KC-BERT Training Epoch 3/3: 100%|██████████| 63/63 [04:19<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 Loss: 0.6223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Ensemble Model: 100%|██████████| 63/63 [03:30<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model - Accuracy: 0.5240, Precision: 0.5165, Recall: 0.7520, F1-Score: 0.6124\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G-01\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Summarizing Reviews:  17%|█▋        | 21/125 [2:20:42<11:36:52, 402.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 230\u001b[0m\n\u001b[0;32m    227\u001b[0m batch \u001b[38;5;241m=\u001b[39m text_data[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m    228\u001b[0m inputs \u001b[38;5;241m=\u001b[39m bart_tokenizer(batch, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 230\u001b[0m summary_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbart_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m summaries \u001b[38;5;241m=\u001b[39m bart_tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(summary_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review, summary \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch, summaries):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\generation\\utils.py:2063\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2055\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2056\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2057\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2058\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2059\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2060\u001b[0m     )\n\u001b[0;32m   2062\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2063\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2064\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2070\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2077\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2078\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2084\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2085\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\generation\\utils.py:3238\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   3235\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[0;32m   3237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[1;32m-> 3238\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3241\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1641\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1637\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1638\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1639\u001b[0m         )\n\u001b[1;32m-> 1641\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1659\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1660\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1527\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1520\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1521\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1522\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1523\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1524\u001b[0m     )\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1379\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1367\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1368\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         use_cache,\n\u001b[0;32m   1377\u001b[0m     )\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1379\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1392\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bart\\modeling_bart.py:665\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    663\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    672\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bart\\modeling_bart.py:470\u001b[0m, in \u001b[0;36mBartSdpaAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    468\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[0;32m    469\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[1;32m--> 470\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# self_attention\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import concurrent.futures\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BartTokenizer, BartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CUDA 오류 발생 시 정확한 위치를 파악하기 위해 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device set to: {device}\")\n",
    "\n",
    "# 1. 데이터 전처리 및 탐색\n",
    "\n",
    "# 데이터셋 로드\n",
    "file_path = './외국음식전문점.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 결측값 처리\n",
    "df['content'] = df['content'].fillna('')\n",
    "\n",
    "# 데이터 샘플링 (빠른 테스트를 위해)\n",
    "df = df.sample(1000, random_state=42)\n",
    "text_data = df['content'].tolist()\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "    text = repeat_normalize(text, num_repeats=2)\n",
    "    stopwords = ['이', '그', '저', '의', '을', '를', '은', '는', '에', '와', '과', '도', '으로', '그리고', '하지만', '그래서']\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text.strip()\n",
    "\n",
    "# 전역적으로 LTokenizer 초기화\n",
    "tokenizer = LTokenizer()\n",
    "\n",
    "# 전처리 및 토큰화 함수\n",
    "def preprocess_and_tokenize(review):\n",
    "    preprocessed_review = preprocess_text(review)\n",
    "    tokens = tokenizer.tokenize(preprocessed_review)\n",
    "    return tokens\n",
    "\n",
    "# 병렬 처리 함수\n",
    "def parallel_process(data, func, num_workers=None):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_data = {executor.submit(func, d): d for d in data}\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_data), total=len(data), desc=\"Preprocessing and Tokenizing Reviews\"):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as exc:\n",
    "                print(f'Generated an exception: {exc}')\n",
    "                results.append(None)\n",
    "    return results\n",
    "\n",
    "# 병렬 처리 수행\n",
    "preprocessed_data = parallel_process(text_data, preprocess_and_tokenize, num_workers=4)\n",
    "print(\"Tokenization completed successfully.\")\n",
    "\n",
    "# 2. 카테고리화 및 키워드 추출\n",
    "\n",
    "# TF-IDF를 사용하여 리뷰 텍스트 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([' '.join(tokens) for tokens in preprocessed_data])\n",
    "\n",
    "# KMeans 클러스터링을 통해 음식 종류 카테고리화\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df['category'] = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# LDA 모델을 사용한 토픽 모델링\n",
    "dictionary = Dictionary(preprocessed_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in preprocessed_data]\n",
    "\n",
    "num_topics = 10\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "topics = lda_model.print_topics(num_words=4)\n",
    "print(\"LDA Model Training Completed.\")\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# 각 클러스터(카테고리)별 키워드 추출\n",
    "for i in range(num_clusters):\n",
    "    cluster_center = kmeans.cluster_centers_[i]\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_terms = [terms[idx] for idx in cluster_center.argsort()[-10:]]\n",
    "    print(f\"Cluster {i} top terms: {', '.join(top_terms)}\")\n",
    "\n",
    "# 3. 모델 구축 및 감성 분석\n",
    "\n",
    "# KoBERT 및 KC-BERT 모델 로드\n",
    "tokenizer_kobert = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model_kobert = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2).to(device)\n",
    "\n",
    "tokenizer_kcbert = BertTokenizer.from_pretrained('beomi/kcbert-base')\n",
    "model_kcbert = BertForSequenceClassification.from_pretrained('beomi/kcbert-base', num_labels=2).to(device)\n",
    "\n",
    "# 감성 분석을 위한 데이터 준비\n",
    "inputs_kobert = tokenizer_kobert(text_data, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "inputs_kcbert = tokenizer_kcbert(text_data, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "\n",
    "labels = torch.tensor([0, 1] * (inputs_kobert['input_ids'].size(0) // 2 + 1))[:inputs_kobert['input_ids'].size(0)].to(device)\n",
    "dataset_kobert = TensorDataset(inputs_kobert['input_ids'], inputs_kobert['attention_mask'], labels)\n",
    "dataset_kcbert = TensorDataset(inputs_kcbert['input_ids'], inputs_kcbert['attention_mask'], labels)\n",
    "\n",
    "train_loader_kobert = DataLoader(dataset_kobert, batch_size=16, shuffle=True)\n",
    "train_loader_kcbert = DataLoader(dataset_kcbert, batch_size=16, shuffle=True)\n",
    "\n",
    "optimizer_kobert = torch.optim.AdamW(model_kobert.parameters(), lr=1e-5)\n",
    "optimizer_kcbert = torch.optim.AdamW(model_kcbert.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# KoBERT 학습\n",
    "for epoch in range(3):  # 예시로 3 에포크 설정\n",
    "    model_kobert.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader_kobert, desc=f\"KoBERT Training Epoch {epoch+1}/3\"):\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        optimizer_kobert.zero_grad()\n",
    "        outputs_kobert = model_kobert(input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs_kobert.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer_kobert.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/3 Loss: {epoch_loss/len(train_loader_kobert):.4f}\")\n",
    "\n",
    "# KC-BERT 학습\n",
    "for epoch in range(3):  # 예시로 3 에포크 설정\n",
    "    model_kcbert.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader_kcbert, desc=f\"KC-BERT Training Epoch {epoch+1}/3\"):\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        optimizer_kcbert.zero_grad()\n",
    "        outputs_kcbert = model_kcbert(input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs_kcbert.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer_kcbert.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/3 Loss: {epoch_loss/len(train_loader_kcbert):.4f}\")\n",
    "\n",
    "# 4. 앙상블 예측\n",
    "def ensemble_predict(kobert_outputs, kcbert_outputs):\n",
    "    ensemble_logits = (kobert_outputs.logits + kcbert_outputs.logits) / 2\n",
    "    return torch.argmax(ensemble_logits, dim=1)\n",
    "\n",
    "# 평가 모드로 전환\n",
    "model_kobert.eval()\n",
    "model_kcbert.eval()\n",
    "\n",
    "ensemble_predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader_kobert, desc=\"Evaluating Ensemble Model\"):\n",
    "        input_ids_kobert, attention_mask_kobert, labels = [x.to(device) for x in batch]\n",
    "        input_ids_kcbert = input_ids_kobert\n",
    "        attention_mask_kcbert = attention_mask_kobert\n",
    "        \n",
    "        outputs_kobert = model_kobert(input_ids_kobert, attention_mask=attention_mask_kobert)\n",
    "        outputs_kcbert = model_kcbert(input_ids_kcbert, attention_mask=attention_mask_kcbert)\n",
    "        \n",
    "        ensemble_preds = ensemble_predict(outputs_kobert, outputs_kcbert)\n",
    "        \n",
    "        ensemble_predictions.extend(ensemble_preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 성능 평가\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(true_labels, ensemble_predictions)\n",
    "precision = precision_score(true_labels, ensemble_predictions)\n",
    "recall = recall_score(true_labels, ensemble_predictions)\n",
    "f1 = f1_score(true_labels, ensemble_predictions)\n",
    "\n",
    "print(f\"Ensemble Model - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# 성능 결과 시각화\n",
    "metrics = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Score': [accuracy, precision, recall, f1]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Metric', y='Score', data=metrics_df)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Ensemble Model Performance')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "\n",
    "# 시각화 결과 저장\n",
    "plt.savefig('ensemble_model_performance.png')\n",
    "plt.show()\n",
    "\n",
    "# 5. 리뷰 요약\n",
    "\n",
    "# BART 모델 로드\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "\n",
    "# 리뷰 요약 결과를 저장할 리스트\n",
    "review_summaries = []\n",
    "\n",
    "# 배치 크기 설정\n",
    "batch_size = 8  # 배치 크기를 8로 설정하여 메모리 사용량 줄이기\n",
    "\n",
    "bart_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(text_data), batch_size), desc=\"Summarizing Reviews\"):\n",
    "        batch = text_data[i:i + batch_size]\n",
    "        inputs = bart_tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=1024).to(device)\n",
    "        \n",
    "        summary_ids = bart_model.generate(\n",
    "            inputs['input_ids'], \n",
    "            attention_mask=inputs['attention_mask'], \n",
    "            max_length=150, \n",
    "            min_length=40, \n",
    "            length_penalty=2.0, \n",
    "            num_beams=4, \n",
    "            early_stopping=True\n",
    "        )\n",
    "        summaries = bart_tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "        \n",
    "        for review, summary in zip(batch, summaries):\n",
    "            review_summaries.append({'Original': review, 'Summary': summary})\n",
    "        \n",
    "        # 메모리 정리\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 리뷰 요약 결과를 파일로 저장\n",
    "review_summaries_df = pd.DataFrame(review_summaries)\n",
    "review_summaries_df.to_csv('review_summaries.csv', index=False)\n",
    "\n",
    "print(\"Review summaries saved to review_summaries.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 성능 결과 시각화\n",
    "metrics = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Score': [accuracy, precision, recall, f1]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Metric', y='Score', data=metrics_df)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Ensemble Model Performance')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "\n",
    "# 시각화 결과 저장\n",
    "plt.savefig('ensemble_model_performance.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G-01\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Summarizing Reviews:   0%|          | 0/6665 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m batch \u001b[38;5;241m=\u001b[39m text_data[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m     36\u001b[0m inputs \u001b[38;5;241m=\u001b[39m bart_tokenizer(batch, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 38\u001b[0m summary_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbart_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m summaries \u001b[38;5;241m=\u001b[39m bart_tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(summary_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review, summary \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch, summaries):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\generation\\utils.py:2063\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2055\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2056\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2057\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2058\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2059\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2060\u001b[0m     )\n\u001b[0;32m   2062\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2063\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2064\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2070\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2077\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2078\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2084\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2085\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\generation\\utils.py:3238\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   3235\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[0;32m   3237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[1;32m-> 3238\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3241\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1641\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1637\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1638\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1639\u001b[0m         )\n\u001b[1;32m-> 1641\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1659\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1660\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1527\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1520\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1521\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1522\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1523\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1524\u001b[0m     )\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1379\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1367\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1368\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         use_cache,\n\u001b[0;32m   1377\u001b[0m     )\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1379\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1392\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bart\\modeling_bart.py:665\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    663\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    672\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bart\\modeling_bart.py:471\u001b[0m, in \u001b[0;36mBartSdpaAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    469\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[0;32m    470\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m0\u001b[39m], key_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 471\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# self_attention\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# CUDA 오류 발생 시 정확한 위치를 파악하기 위해 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device set to: {device}\")\n",
    "\n",
    "# 데이터셋 로드\n",
    "file_path = './외국음식전문점.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 결측값 처리\n",
    "df['content'] = df['content'].fillna('')\n",
    "text_data = df['content'].tolist()\n",
    "\n",
    "# BART 모델 로드\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "\n",
    "# 리뷰 요약 결과를 저장할 리스트\n",
    "review_summaries = []\n",
    "\n",
    "# 배치 크기 설정\n",
    "batch_size = 8  # 배치 크기를 8로 설정하여 메모리 사용량 줄이기\n",
    "\n",
    "bart_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(text_data), batch_size), desc=\"Summarizing Reviews\"):\n",
    "        batch = text_data[i:i + batch_size]\n",
    "        inputs = bart_tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=1024).to(device)\n",
    "        \n",
    "        summary_ids = bart_model.generate(\n",
    "            inputs['input_ids'], \n",
    "            attention_mask=inputs['attention_mask'], \n",
    "            max_length=150, \n",
    "            min_length=40, \n",
    "            length_penalty=2.0, \n",
    "            num_beams=4, \n",
    "            early_stopping=True\n",
    "        )\n",
    "        summaries = bart_tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "        \n",
    "        for review, summary in zip(batch, summaries):\n",
    "            review_summaries.append({'Original': review, 'Summary': summary})\n",
    "        \n",
    "        # 메모리 정리\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 리뷰 요약 결과를 파일로 저장\n",
    "review_summaries_df = pd.DataFrame(review_summaries)\n",
    "review_summaries_df.to_csv('review_summaries.csv', index=False)\n",
    "\n",
    "print(\"Review summaries saved to review_summaries.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
