{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.737 Gbory 0.492 Gb\n",
      "all cohesion probabilities was computed. # words = 46096\n",
      "all branching entropies was computed # words = 62150\n",
      "all accessor variety was computed # words = 62150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Reviews with Soynlp: 100%|██████████| 51124/51124 [00:02<00:00, 18365.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed successfully.\n",
      "LDA Model Training Completed.\n",
      "(0, '0.034*\"샐러\" + 0.012*\"드\" + 0.010*\"스프\" + 0.008*\"케이크\"')\n",
      "(1, '0.013*\"케밥\" + 0.013*\"아요\" + 0.012*\"괜찮\" + 0.009*\"싹싹\"')\n",
      "(2, '0.022*\"분위기\" + 0.019*\"좋은\" + 0.017*\"파스타\" + 0.016*\"한\"')\n",
      "(3, '0.039*\"맛나요\" + 0.017*\"굳\" + 0.008*\"디아\" + 0.008*\"퀘사\"')\n",
      "(4, '0.080*\"맛있\" + 0.054*\"도\" + 0.049*\"너무\" + 0.033*\"친절\"')\n",
      "(5, '0.198*\"맛있\" + 0.160*\"어요\" + 0.015*\"어요!\" + 0.010*\"너무\"')\n",
      "(6, '0.016*\"방문\" + 0.014*\"이\" + 0.013*\"너무\" + 0.013*\"음식\"')\n",
      "(7, '0.142*\"굿\" + 0.020*\"굿굿\" + 0.009*\"바삭\" + 0.008*\"쏘쏘\"')\n",
      "(8, '0.114*\"좋아요\" + 0.009*\"맛잇어요\" + 0.007*\"+\" + 0.006*\"밥은\"')\n",
      "(9, '0.042*\"맛있\" + 0.022*\"쌀국수\" + 0.017*\"가\" + 0.017*\"는\"')\n",
      "Device set to: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "C:\\Users\\G-01\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "KoBERT Training Epoch 1/3:   0%|          | 0/3196 [00:00<?, ?it/s]C:\\Users\\G-01\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "KoBERT Training Epoch 1/3: 100%|██████████| 3196/3196 [1:34:15<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KoBERT Training Epoch 2/3: 100%|██████████| 3196/3196 [1:34:13<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KoBERT Training Epoch 3/3: 100%|██████████| 3196/3196 [1:34:03<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "KC-BERT Training Epoch 1/3: 100%|██████████| 3196/3196 [5:13:03<00:00,  5.88s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 0.7035132192998416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KC-BERT Training Epoch 2/3:  10%|▉         | 305/3196 [28:29<4:30:08,  5.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m epoch_loss_kcbert \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader_kcbert, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKC-BERT Training Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 84\u001b[0m     input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     85\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     86\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_kcbert(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n",
      "Cell \u001b[1;32mIn[1], line 84\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m epoch_loss_kcbert \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader_kcbert, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKC-BERT Training Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 84\u001b[0m     input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m [\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     85\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     86\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_kcbert(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BartTokenizer, BartForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "file_path = './외국음식전문점.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "text_data = df['content'].dropna().tolist()\n",
    "\n",
    "# Soynlp LTokenizer 사용\n",
    "from soynlp.word import WordExtractor\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(text_data)\n",
    "word_scores = word_extractor.extract()\n",
    "float_word_scores = {word: score.cohesion_forward for word, score in word_scores.items()}\n",
    "\n",
    "# LTokenizer를 사용해 텍스트 토큰화\n",
    "tokenizer = LTokenizer(scores=float_word_scores)\n",
    "tokenized_data = []\n",
    "for review in tqdm(text_data, desc=\"Tokenizing Reviews with Soynlp\"):\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    tokenized_data.append(tokens)\n",
    "\n",
    "print(\"Tokenization completed successfully.\")\n",
    "\n",
    "# 2. LDA 모델을 사용한 토픽 모델링\n",
    "dictionary = Dictionary(tokenized_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    "\n",
    "num_topics = 10\n",
    "passes = 15\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "print(\"LDA Model Training Completed.\")\n",
    "topics = lda_model.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# 3. KoBERT를 사용한 감성 분석\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device set to: {device}\")\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model_kobert = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2).to(device)\n",
    "\n",
    "inputs = tokenizer_bert(text_data, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "labels = torch.tensor([0] * len(text_data))  # 예시 레이블 설정 (실제 데이터 사용 필요)\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_kobert.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model_kobert.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"KoBERT Training Epoch {epoch+1}/{epochs}\")):\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_kobert(input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 4. KC-BERT를 사용한 신조어 처리 및 감성 분석\n",
    "tokenizer_kcbert = BertTokenizer.from_pretrained('beomi/kcbert-base')\n",
    "model_kcbert = BertForSequenceClassification.from_pretrained('beomi/kcbert-base', num_labels=2).to(device)\n",
    "\n",
    "inputs_kcbert = tokenizer_kcbert(text_data, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "labels = torch.tensor([0, 1] * (inputs_kcbert['input_ids'].size(0) // 2 + 1))[:inputs_kcbert['input_ids'].size(0)]\n",
    "dataset_kcbert = TensorDataset(inputs_kcbert['input_ids'], inputs_kcbert['attention_mask'], labels)\n",
    "train_loader_kcbert = DataLoader(dataset_kcbert, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_kcbert.train()\n",
    "    epoch_loss_kcbert = 0\n",
    "    for batch in tqdm(train_loader_kcbert, desc=f\"KC-BERT Training Epoch {epoch+1}/{epochs}\"):\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_kcbert(input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        epoch_loss_kcbert += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss_kcbert/len(train_loader_kcbert)}\")\n",
    "\n",
    "# 5. BART 모델을 사용한 리뷰 요약\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "\n",
    "# NaN 값을 빈 문자열로 대체하고, 모든 값을 문자열로 변환\n",
    "df['content'] = df['content'].fillna('').astype(str)\n",
    "grouped_reviews = df.groupby('store_name')['content'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# 각 가게별 리뷰를 저장할 리스트\n",
    "store_summaries = []\n",
    "bart_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, row in tqdm(grouped_reviews.iterrows(), desc=\"Summarizing Reviews by Store\", total=len(grouped_reviews)):\n",
    "        store_name = row['store_name']\n",
    "        combined_reviews = row['content']\n",
    "\n",
    "        # 가게에 대한 모든 리뷰를 하나의 입력으로 요약 (간단 요약)\n",
    "        inputs = bart_tokenizer(combined_reviews, return_tensors='pt', max_length=1024, truncation=True)\n",
    "        inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "        \n",
    "        # 간단한 문장 요약\n",
    "        summary_ids_sentence = bart_model.generate(\n",
    "            inputs['input_ids'], \n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=50,  # 간단한 문장 요약\n",
    "            min_length=20, \n",
    "            length_penalty=2.0, \n",
    "            num_beams=4, \n",
    "            early_stopping=True\n",
    "        )\n",
    "        summary_sentence = bart_tokenizer.decode(summary_ids_sentence[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 상세한 문단 요약\n",
    "        summary_ids_paragraph = bart_model.generate(\n",
    "            inputs['input_ids'], \n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=150,  # 상세한 문단 요약\n",
    "            min_length=60, \n",
    "            length_penalty=2.0, \n",
    "            num_beams=4, \n",
    "            early_stopping=True\n",
    "        )\n",
    "        summary_paragraph = bart_tokenizer.decode(summary_ids_paragraph[0], skip_special_tokens=True)\n",
    "        \n",
    "        store_summaries.append({\n",
    "            'store_name': store_name, \n",
    "            'summary_sentence': summary_sentence,\n",
    "            'summary_paragraph': summary_paragraph\n",
    "        })\n",
    "        \n",
    "        # 콘솔에 요약 결과 출력\n",
    "        print(f\"Store: {store_name}\\nSummary (Sentence): {summary_sentence}\\nSummary (Paragraph): {summary_paragraph}\\n\")\n",
    "\n",
    "# 가게별 요약 결과를 파일로 저장\n",
    "store_summaries_df = pd.DataFrame(store_summaries)\n",
    "store_summaries_df.to_csv(\"store_review_summaries.csv\", index=False)\n",
    "print(\"Store review summaries saved to store_review_summaries.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
