{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CUDA 오류 발생 시 정확한 위치를 파악하기 위해 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device set to: {device}\")\n",
    "\n",
    "# 데이터셋 로드 및 전처리\n",
    "file_path = './외국음식전문점.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['content'] = df['content'].fillna('')\n",
    "df = df.sample(500, random_state=42)  # 샘플 데이터로 500개만 사용\n",
    "\n",
    "text_data = df['content'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "    text = repeat_normalize(text, num_repeats=2)\n",
    "    stopwords = ['이', '그', '저', '의', '을', '를', '은', '는', '에', '와', '과', '도', '으로', '그리고', '하지만', '그래서']\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text.strip()\n",
    "\n",
    "tokenizer = LTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 전처리 및 토큰화 함수\n",
    "def preprocess_and_tokenize(review):\n",
    "    preprocessed_review = preprocess_text(review)\n",
    "    tokens = tokenizer.tokenize(preprocessed_review)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "C:\\Users\\G-01\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# KoBERT 및 KC-BERT 모델 로드\n",
    "tokenizer_kobert = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model_kobert = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2).to(device)\n",
    "\n",
    "tokenizer_kcbert = BertTokenizer.from_pretrained('beomi/kcbert-base')\n",
    "model_kcbert = BertForSequenceClassification.from_pretrained('beomi/kcbert-base', num_labels=2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33e7fc453e34e1ead81b4433dacf08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/682k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cc3b2961704396b1b5de3cf8be1efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e88e1b401bb4cada34b906246f4deb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f86366d7614d929b16a70ab3407f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d494f2d613a24af58e71cccd6959fdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/495M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# KoBART 모델 로드\n",
    "bart_tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v1').to(device)\n",
    "\n",
    "# 감성 분석과 요약 결과를 저장할 리스트\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Reviews: 100%|██████████| 500/500 [15:35<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to kobert_kcbert_kobart_results_fixed_final.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 감성 분석과 요약을 동시에 수행\n",
    "for review in tqdm(text_data, desc=\"Processing Reviews\"):\n",
    "    review = review.strip()\n",
    "    if len(review) < 10:  # 리뷰가 너무 짧으면 건너뜁니다.\n",
    "        continue\n",
    "\n",
    "    # 감성 분석 (KoBERT, KC-BERT)\n",
    "    inputs_kobert = tokenizer_kobert(review, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "    inputs_kcbert = tokenizer_kcbert(review, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            output_kobert = model_kobert(**inputs_kobert)\n",
    "            output_kcbert = model_kcbert(**inputs_kcbert)\n",
    "\n",
    "        sentiment_kobert = torch.argmax(output_kobert.logits, dim=1).item()\n",
    "        sentiment_kcbert = torch.argmax(output_kcbert.logits, dim=1).item()\n",
    "\n",
    "        # 리뷰 요약 (KoBART)\n",
    "        inputs_bart = bart_tokenizer([review], return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            summary_ids = bart_model.generate(\n",
    "                inputs_bart['input_ids'], \n",
    "                attention_mask=inputs_bart['attention_mask'], \n",
    "                max_length=100,  \n",
    "                min_length=20,   \n",
    "                length_penalty=1.5,  \n",
    "                num_beams=4, \n",
    "                no_repeat_ngram_size=3,  # n-그램 반복 방지\n",
    "                repetition_penalty=1.2,  # 반복에 대한 패널티 부여\n",
    "                early_stopping=True\n",
    "            )\n",
    "        summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Runtime error with review: {review}\")\n",
    "        print(e)\n",
    "        continue  # 에러가 발생하면 해당 리뷰는 건너뜁니다.\n",
    "\n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        'Original Review': review,\n",
    "        'KoBERT Sentiment': sentiment_kobert,\n",
    "        'KC-BERT Sentiment': sentiment_kcbert,\n",
    "        'KoBART Summary': summary\n",
    "    })\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('kobert_kcbert_kobart_results_fixed_final.csv', index=False)\n",
    "\n",
    "print(\"Results saved to kobert_kcbert_kobart_results_fixed_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to kobert_kcbert_kobart_results.csv\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
